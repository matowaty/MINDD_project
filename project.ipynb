{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Bank Telemarketing Campaign - Predictive Modeling Project\n",
    "\n",
    "**Project Goal:** Build data-driven models to predict the success of telemarketing calls for long-term bank deposits\n",
    "\n",
    "**Dataset Period:** 2008-2013 (Global Financial Crisis)\n",
    "\n",
    "**Methodology:** CRISP-DM (Cross-Industry Standard Process for Data Mining)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Business Objectives\n",
    "- TODO: Define the business problem\n",
    "- TODO: Identify key stakeholders\n",
    "- TODO: Define success criteria for the project\n",
    "\n",
    "### 1.2 Project Goals\n",
    "- TODO: Translate business objectives into data mining goals\n",
    "- TODO: Define target variable\n",
    "- TODO: Identify evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC)\n",
    "\n",
    "### 1.3 Business Context\n",
    "- TODO: Describe the telemarketing campaign process\n",
    "- TODO: Explain the financial crisis context (2008-2013)\n",
    "- TODO: Define constraints and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# TODO: Add imports as needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# TODO: Add scikit-learn imports\n",
    "# TODO: Add any other libraries needed\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Understanding\n",
    "\n",
    "### 2.1 Data Collection\n",
    "- TODO: Load the dataset - DONE\n",
    "- TODO: Document data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# TODO: Load bank.csv or appropriate data file\n",
    "df = pd.read_csv('bank.csv', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.2 Data Description\n",
    "- TODO: Examine dataset structure\n",
    "- TODO: Identify features and their types\n",
    "- TODO: Document feature definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "# TODO: df.info()\n",
    "# TODO: df.describe()\n",
    "# TODO: df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.3 Data Exploration\n",
    "- TODO: Analyze target variable distribution - DONE\n",
    "- TODO: Check for class imbalance - DONE\n",
    "- TODO: Explore feature distributions - DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "# TODO: Analyze subscription rates\n",
    "# TODO: Visualize class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The target variable 'y' represents whether a customer will buy a long-term bank deposit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target value\n",
    "goal = df['y']\n",
    "counts = goal.value_counts()\n",
    "percent = goal.value_counts(normalize=True)\n",
    "percent100 = goal.value_counts(normalize=True).mul(100).round(1).astype(str)+'%'\n",
    "pd.DataFrame({'y': counts,'percent': percent100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_summary = pd.DataFrame({\n",
    "    'class': counts.index,\n",
    "    'count': counts.values,\n",
    "    'percent': percent.values\n",
    "})\n",
    "\n",
    "print(subscription_summary)\n",
    "\n",
    "sns.countplot(data=df, x='y')\n",
    "plt.title('Subscription Class Distribution')\n",
    "plt.xlabel('Subscription (y)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_ratio = counts.min() / counts.max()\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Imbalance ratio is below 0.2 which means there's a severe imbalance. Dataset is dominated by non\n",
    "Without adressing this imbalance, predictive models predicting \"no purchase\" would achieve high accuracy, but fail to identify potential buyers.\n",
    "\n",
    "To handle class imbalance we will use SMOTE combined with Tomek Links technique later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis\n",
    "# TODO: Analyze numerical features\n",
    "# TODO: Analyze categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "numericFeatures = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categFeatures = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Numerical features:\", numericFeatures)\n",
    "print(\"Categorical features:\", categFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis - shouldnt it be later???\n",
    "# TODO: Analyze relationships with target variable\n",
    "# TODO: Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 2.4 Data Quality Assessment\n",
    "- TODO: Check for missing values - DONE\n",
    "- TODO: Identify outliers - DONE\n",
    "- TODO: Check for duplicates - DONE\n",
    "- TODO: Identify data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "# TODO: df.isnull().sum()\n",
    "# TODO: df.duplicated().sum()\n",
    "# TODO: Check for outliers using statistical methods or visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % missing values by column\n",
    "\n",
    "nulls = df.isnull().sum()\n",
    "percent = round(nulls/df.shape[0]*100,3)\n",
    "nullvalues = pd.concat([nulls,percent], axis=1, keys=('Cont','%'))\n",
    "nullvalues\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unknown():\n",
    "    categorical_cols_with_unknown = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "# Count 'unknown' in each column\n",
    "    unknown_counts = {col: (df[col] == 'unknown').sum() for col in categorical_cols_with_unknown}\n",
    "    unknown_df = pd.DataFrame.from_dict(unknown_counts, orient='index', columns=['Count'])\n",
    "    unknown_df['Percent'] = round(unknown_df['Count'] / df.shape[0] * 100, 3)\n",
    "    print(unknown_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "There are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_plots = len(numericFeatures)\n",
    "cols = 2\n",
    "rows = math.ceil(num_plots / cols)\n",
    "\n",
    "plt.figure(figsize=(cols * 4, rows * 5)) \n",
    "\n",
    "for i, feature in enumerate(numericFeatures):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    sns.boxplot(data=df, x='y', y=feature, hue='y', palette={\"yes\": \"red\", \"no\": \"green\"})\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df[numericFeatures].quantile(0.25)\n",
    "Q3 = df[numericFeatures].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = ((df[numericFeatures] < (Q1 - 1.5 * IQR)) | (df[numericFeatures] > (Q3 + 1.5 * IQR)))\n",
    "print(\"Number of outliers per numeric feature:\")\n",
    "print(outliers.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We performed outliers detection on all numerical features using the Interquartile Range (IQR) method and visualized it using boxplots.\n",
    "\n",
    "Features such as *previous*, *duration* and *campaign* contained a high number of outlier values.\n",
    "*age* and *pdays* alco contained a noticeable number out outliers.\n",
    "Several features, including *emp.var.rate*, *cons.price.idx*, e8uribor3m* and *nr.employed*, showed no outliers according to the IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate lines, if they exist\n",
    "\n",
    "shape_before = df.shape\n",
    "print('Shape before deleting duplicate values:',shape_before)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "shape_after = df.shape\n",
    "print('Shape after deleting duplicate values:',shape_after)\n",
    "\n",
    "percent = round((1-shape_after[0]/shape_before[0])*100,3)\n",
    "print(f\"Percentage of duplicates rows droped: {percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "There are 12 duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Data Cleaning\n",
    "- TODO: Handle missing values - DONE\n",
    "- TODO: Remove/treat outliers - We will use Robust Scaler.\n",
    "- TODO: Remove duplicates - DONE\n",
    "- TODO: Fix data inconsistencies - DONE (there were none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_cols = ['job', 'marital', 'education']\n",
    "\n",
    "for col in impute_cols:\n",
    "    most_common = df.loc[df[col] != 'unknown', col].mode()[0]\n",
    "    df.loc[df[col] == 'unknown', col] = most_common\n",
    "    print(f\"Imputed 'unknown' in '{col}' with: {most_common}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unknown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_cols = ['default', 'housing', 'loan']\n",
    "\n",
    "for col in replace_cols:\n",
    "    df.loc[df[col] == 'unknown', col] = 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unknown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate lines, if they exist\n",
    "\n",
    "shape_before = df.shape\n",
    "print('Shape before deleting duplicate values:',shape_before)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "shape_after = df.shape\n",
    "print('Shape after deleting duplicate values:',shape_after)\n",
    "\n",
    "percent = round((1-shape_after[0]/shape_before[0])*100,3)\n",
    "print(f\"Percentage of duplicates rows droped: {percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object').columns:\n",
    "    print(f\"{col} unique values: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 3.2 Data Transformation\n",
    "- TODO: Encode categorical variables - DONE\n",
    "- TODO: Scale/normalize numerical features - DONE\n",
    "- TODO: Handle skewed distributions - DONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "# TODO: Apply one-hot encoding, label encoding, or ordinal encoding as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = df.copy()\n",
    "\n",
    "for feature in categFeatures:\n",
    "    print(feature)\n",
    "    print(dfML[feature].unique())\n",
    "    if dfML[feature].dropna().isin(['yes', 'no']).all():\n",
    "        dfML[feature] = (dfML[feature].values == 'yes').astype(int)\n",
    "        print(dfML[feature].unique())\n",
    "\n",
    "dfML.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = pd.get_dummies(dfML, drop_first=True, dtype=int)\n",
    "\n",
    "dfML.rename({'y_yes': 'y'}, axis='columns', inplace = True)\n",
    "\n",
    "dfML.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "# TODO: Apply StandardScaler, MinMaxScaler, or other scaling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "                    'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler on the numeric features and transform\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 3.3 Feature Engineering\n",
    "- TODO: Create new features from existing ones\n",
    "- TODO: Create interaction features\n",
    "- TODO: Create time-based features if applicable\n",
    "- TODO: Create domain-specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# TODO: Create new features based on domain knowledge and EDA insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### 3.4 Feature Selection\n",
    "- TODO: Identify highly correlated features\n",
    "- TODO: Apply feature importance analysis\n",
    "- TODO: Select relevant features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# TODO: Implement feature selection techniques\n",
    "# (correlation analysis, feature importance, recursive feature elimination, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### 3.5 Data Splitting\n",
    "- TODO: Split data into training and test sets - DONE\n",
    "- TODO: Handle class imbalance if necessary (SMOTE, undersampling, etc.) - DONE \n",
    "- TODO: Set up cross-validation strategy ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "# TODO: from sklearn.model_selection import train_test_split\n",
    "# TODO: X_train, X_test, y_train, y_test = train_test_split(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Original dataframe\n",
    "# Combine features and target for training set\n",
    "train_df = X_train.copy()\n",
    "train_df['y'] = y_train\n",
    "\n",
    "# Combine features and target for testing set\n",
    "test_df = X_test.copy()\n",
    "test_df['y'] = y_test\n",
    "\n",
    "\n",
    "# This second dataframe is processed\n",
    "\n",
    "X = dfML.drop(columns=['y'])\n",
    "y = dfML['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance\n",
    "# TODO: Apply SMOTE, class weights, or other techniques if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfML = X_train_resampled.copy()\n",
    "train_dfML['y'] = y_train_resampled\n",
    "\n",
    "test_dfML = X_test.copy()\n",
    "test_dfML['y'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Modeling\n",
    "\n",
    "### 4.1 Baseline Model\n",
    "- TODO: Create a simple baseline model (e.g., majority class classifier)\n",
    "- TODO: Evaluate baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "# TODO: Implement baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 4.2 Model Selection\n",
    "- TODO: Train multiple algorithms from class:\n",
    "  - Logistic Regression\n",
    "  - Decision Trees\n",
    "  - Random Forest\n",
    "  - Gradient Boosting (XGBoost, LightGBM)\n",
    "  - Support Vector Machines\n",
    "  - Neural Networks\n",
    "  - K-Nearest Neighbors\n",
    "  - Naive Bayes\n",
    "  - TODO: Add others as covered in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "# TODO: Train and evaluate logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Tree\n",
    "# TODO: Train and evaluate decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest\n",
    "# TODO: Train and evaluate random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Gradient Boosting\n",
    "# TODO: Train and evaluate gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Support Vector Machine\n",
    "# TODO: Train and evaluate SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6: [Add more models as needed]\n",
    "# TODO: Train and evaluate additional models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### 4.3 Hyperparameter Tuning\n",
    "- TODO: Define hyperparameter search space\n",
    "- TODO: Apply Grid Search or Random Search\n",
    "- TODO: Use cross-validation for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - Model 1\n",
    "# TODO: Implement GridSearchCV or RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - Model 2\n",
    "# TODO: Implement hyperparameter tuning for other promising models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 4.4 Ensemble Methods\n",
    "- TODO: Create ensemble models (voting, stacking, blending)\n",
    "- TODO: Combine best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble models\n",
    "# TODO: Implement ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation\n",
    "\n",
    "### 5.1 Model Performance Metrics\n",
    "- TODO: Calculate accuracy, precision, recall, F1-score\n",
    "- TODO: Generate ROC curves and calculate AUC\n",
    "- TODO: Create confusion matrices\n",
    "- TODO: Calculate business-relevant metrics (cost/benefit analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "# TODO: Calculate and compare all metrics across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "# TODO: Create ROC curves, precision-recall curves\n",
    "# TODO: Create confusion matrices\n",
    "# TODO: Create comparison charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### 5.2 Model Interpretation\n",
    "- TODO: Analyze feature importance\n",
    "- TODO: Interpret model predictions\n",
    "- TODO: Validate model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# TODO: Extract and visualize feature importance from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation\n",
    "# TODO: Use SHAP, LIME, or other interpretation methods if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### 5.3 Model Validation\n",
    "- TODO: Perform cross-validation\n",
    "- TODO: Test on holdout set\n",
    "- TODO: Check for overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "# TODO: Perform k-fold cross-validation on best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation on test set\n",
    "# TODO: Evaluate final model(s) on unseen test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### 5.4 Business Impact Assessment\n",
    "- TODO: Translate model performance to business value\n",
    "- TODO: Calculate expected ROI or cost savings\n",
    "- TODO: Provide actionable recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis\n",
    "# TODO: Calculate business metrics (conversion rate improvement, cost reduction, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Conclusions and Recommendations\n",
    "\n",
    "### 6.1 Summary of Findings\n",
    "- TODO: Summarize key insights from data exploration\n",
    "- TODO: Summarize model performance\n",
    "- TODO: Identify most important predictive features\n",
    "\n",
    "### 6.2 Best Model Selection\n",
    "- TODO: Select and justify the best model\n",
    "- TODO: Document model strengths and limitations\n",
    "\n",
    "### 6.3 Recommendations\n",
    "- TODO: Provide actionable business recommendations\n",
    "- TODO: Suggest customer prioritization strategy\n",
    "- TODO: Recommend campaign optimization strategies\n",
    "\n",
    "### 6.4 Future Work\n",
    "- TODO: Suggest model improvements\n",
    "- TODO: Identify additional data needs\n",
    "- TODO: Propose deployment strategy\n",
    "\n",
    "### 6.5 Lessons Learned\n",
    "- TODO: Document challenges faced\n",
    "- TODO: Share insights from the project\n",
    "- TODO: Note what would be done differently\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Project Notes and Team Collaboration\n",
    "\n",
    "### Team Members\n",
    "- TODO: List team members and responsibilities\n",
    "\n",
    "### Project Timeline\n",
    "- TODO: Document project milestones and deadlines\n",
    "\n",
    "### References\n",
    "- TODO: Add references to papers, documentation, and resources used\n",
    "\n",
    "---\n",
    "*This notebook follows the CRISP-DM methodology for data mining projects*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
