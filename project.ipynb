{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Bank Telemarketing Campaign - Predictive Modeling Project\n",
    "\n",
    "**Project Goal:** Build data-driven models to predict the success of telemarketing calls for long-term bank deposits\n",
    "\n",
    "**Dataset Period:** 2008-2013 (Global Financial Crisis)\n",
    "\n",
    "**Methodology:** CRISP-DM (Cross-Industry Standard Process for Data Mining)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Business Objectives\n",
    "- TODO: Define the business problem\n",
    "- TODO: Identify key stakeholders\n",
    "- TODO: Define success criteria for the project\n",
    "\n",
    "### 1.2 Project Goals\n",
    "- TODO: Translate business objectives into data mining goals\n",
    "- TODO: Define target variable\n",
    "- TODO: Identify evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC)\n",
    "\n",
    "### 1.3 Business Context\n",
    "- TODO: Describe the telemarketing campaign process\n",
    "- TODO: Explain the financial crisis context (2008-2013)\n",
    "- TODO: Define constraints and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# TODO: Add imports as needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import time\n",
    "from scipy.stats import randint\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, average_precision_score, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "# TODO: Add scikit-learn imports\n",
    "# TODO: Add any other libraries needed\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway\n",
    "import math\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Understanding\n",
    "\n",
    "### 2.1 Data Collection\n",
    "\n",
    "The first step is to load the dataset into the working environment. This involves importing the necessary libraries and reading the data file into a suitable data structure, namely, a DataFrame using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('bank.csv', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.2 Data Description \n",
    "??? do we need to describe evetything since we have a file describing the dataframe\n",
    "- TODO: Examine dataset structure - \n",
    "- TODO: Identify features and their types\n",
    "- TODO: Document feature definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### Basic dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "This dataset contains 41188 entries and 21 features, representing information collected from a direct marketing campaign by a Portuguese bank institution. Each row corresponds to a contact with a client and the goal is to predict whether the client purchased the term deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 2.3 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Target variable analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The target variable 'y' represents whether a customer will buy a long-term bank deposit or not.\n",
    "To check class imbalance we will compare the number of records with 'yes' and 'no' values in *y* column and plot a bar plot to depict class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target value\n",
    "goal = df['y']\n",
    "counts = goal.value_counts()\n",
    "percent = goal.value_counts(normalize=True)\n",
    "percent100 = goal.value_counts(normalize=True).mul(100).round(1).astype(str)+'%'\n",
    "pd.DataFrame({'y': counts,'percent': percent100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_summary = pd.DataFrame({\n",
    "    'class': counts.index,\n",
    "    'count': counts.values,\n",
    "    'percent': percent.values\n",
    "})\n",
    "\n",
    "print(subscription_summary)\n",
    "\n",
    "sns.countplot(data=df, x='y')\n",
    "plt.title('Purchase Class Distribution')\n",
    "plt.xlabel('Purchase (y)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "As an additional check, we will calculate imbalance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_ratio = counts.min() / counts.max()\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Imbalance ratio is 0.13 (below 0.2) which means there's a severe imbalance. Dataset is dominated by non-purchase cases.\n",
    "Without adressing this imbalance, predictive models predicting \"no purchase\" would achieve high accuracy, but fail to identify potential buyers.\n",
    "\n",
    "To handle class imbalance we will use SMOTE combined with Tomek Links technique later on in paragraph `3.4.4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Univariate analysis\n",
    "In the next step, we will extract numerical and categorical features from the dataset for separate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericFeatures = list(df.select_dtypes(include='number').columns)\n",
    "categFeatures = list(set(df.columns) - set(numericFeatures))\n",
    "\n",
    "# Remove goal attribute\n",
    "goalAttrib = df.columns.values.tolist()[-1]\n",
    "if goalAttrib in categFeatures:\n",
    "    categFeatures.remove(goalAttrib)\n",
    "elif goalAttrib in numericFeatures:\n",
    "    numericFeatures.remove(goalAttrib)\n",
    "    \n",
    "print(len(numericFeatures),\" Numeric Features: \",numericFeatures)\n",
    "print(len(categFeatures),\" Categorical Features: \",categFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Numerical features: ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "Categorical features: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Feature distribution analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now we will perform feature distribution analysis to examine how the data values are spread across the dataset. We will use plots and histograms to visualise the distributions of categorical and numerical features. Box plots for numerical features will be skipped in this step, as they will be specifically used for outlier detection in paragraph `2.4.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categ_feat_distribution():\n",
    "    nrows = math.ceil(len(categFeatures) / 2)\n",
    "    ncols = 2\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 5))\n",
    "\n",
    "    axes = axes.flatten() if len(categFeatures) > 1 else [axes]\n",
    "\n",
    "    for i, col in enumerate(categFeatures):\n",
    "        df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
    "        axes[i].set_title(f'{col} Counts')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "There are 'unknown' values present in several catagorical features, which represent missing data, which, in turn, distorts statistical summaries. Because of this we will revisit and redo this analysis after `Data Cleaning` to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonDiscreteFeatures = ['duration', 'euribor3m']\n",
    "discreteFeatures = [f for f in numericFeatures if f not in nonDiscreteFeatures]\n",
    "\n",
    "# histograms for continous featueres\n",
    "nrows = math.ceil(len(nonDiscreteFeatures) / 2)\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(nonDiscreteFeatures):\n",
    "    df[col].hist(bins=30, ax=axes[i])\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# bar plot for discrete features\n",
    "nrows = math.ceil(len(discreteFeatures) / 2)\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(discreteFeatures):\n",
    "    counts = df[col].value_counts().sort_index()\n",
    "    counts.plot(kind='bar', ax=axes[i])\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(len(discreteFeatures), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "None of the numerical features has normal distribution. Only *age* distribution is close to normal distribution. *duration*, *campaign* and *previous* are strongly skewed to the right, whereas *euribor3m* and *nr.employed* are very slightly skewed to the left. *emp.var.rare*, *cons.conf.idx* and it can be ignored.\n",
    "Variablies with skewed distribution will be handled in paragraph `3.4.3` to avoid issues with distance-based models used in paragraph `4.2`.\n",
    "\n",
    "*pdays* distribution is not accurate because of '999' value which indicates that the client hasn't been called before. There is a huge spike at 999, which inidicates that many clients are new targets for the campaign. Therefore, for more detailed interpretation, the analysis of this feature will be conducted again after `Data Cleaning`.\n",
    "\n",
    "The data represented by other variables is highly variable with clusters at different value ranges and lots of fluctuations in frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_cols = ['duration', 'campaign', 'previous']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 2.4 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### 2.4.1 Identify missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "In the first place, we will check for missing values to ensure data completeness and avoid potential issues during analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % missing values by column\n",
    "\n",
    "nulls = df.isnull().sum()\n",
    "percent = round(nulls/df.shape[0]*100,3)\n",
    "nullvalues = pd.concat([nulls,percent], axis=1, keys=('Cont','%'))\n",
    "nullvalues\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Since there are no empty cells and 'unknown' value indicates a missing value, `df.isnull()` cannot be used. It will return information that there are no missing values, hence the cells with 'unknown' must be checked manually (and not using a pre-build function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unknown():\n",
    "    categorical_cols_with_unknown = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "# Count 'unknown' in each column\n",
    "    unknown_counts = {col: (df[col] == 'unknown').sum() for col in categorical_cols_with_unknown}\n",
    "    unknown_df = pd.DataFrame.from_dict(unknown_counts, orient='index', columns=['Count'])\n",
    "    unknown_df['Percent'] = round(unknown_df['Count'] / df.shape[0] * 100, 3)\n",
    "    print(unknown_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unknown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Columns *job*, *marital*, *education*, *default*, *housing* and *loan*, as indicated in the `bank-information.txt` file, contain missing values labaled as 'unknown'. \n",
    "\n",
    "The frequency of 'unknown' entries varies across columns, with *default* having the highest proportion at 20.87% (8,597 entries), followed by *education* at 4.20% (1,731 entries), *housing* and *loan* both at 2.40% (990 entries each), *job* at 0.80% (330 entries), and *marital* at 0.19% (80 entries)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### 2.4.2 Identify outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Based on dataframe information in `paragraph 2.2` there might be potential outliers such as clients contacted up to 56 times in the *capmaign* variable. It can disproportionately influence analysis and model results.\n",
    "That's why now we will identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = len(numericFeatures)\n",
    "cols = 2\n",
    "rows = math.ceil(num_plots / cols)\n",
    "\n",
    "plt.figure(figsize=(cols * 4, rows * 5)) \n",
    "\n",
    "for i, feature in enumerate(numericFeatures):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    sns.boxplot(data=df, x='y', y=feature, hue='y', palette={\"yes\": \"red\", \"no\": \"green\"})\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df[numericFeatures].quantile(0.25)\n",
    "Q3 = df[numericFeatures].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = ((df[numericFeatures] < (Q1 - 1.5 * IQR)) | (df[numericFeatures] > (Q3 + 1.5 * IQR)))\n",
    "print(\"Number of outliers per numeric feature:\")\n",
    "print(outliers.sum())\n",
    "\n",
    "outliers_cols = outliers.any()\n",
    "outliers_cols = outliers_cols[outliers_cols].index.tolist()\n",
    "print(\"\\nColumns containing outliers:\")\n",
    "print(outliers_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "We performed outliers detection on all numerical features using the Interquartile Range (IQR) method and visualized it using boxplots.\n",
    "\n",
    "Features such as *previous*, *duration* and *campaign* contained a high number of outlier values.\n",
    "*age* and *pdays* alco contained a noticeable number out outliers.\n",
    "Several features, including *emp.var.rate*, *cons.price.idx*, *e8uribor3m* and *nr.employed*, showed no outliers according to the IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### 2.4.3 Check data duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "The next step is to check for data duplication, because duplicate records can cause biased insights and reduce the effectiveness of the model. Removing duplicates helps maintain the integrity of the data and ensures that each observation contributes uniquely to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "There are 12 duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Data Cleaning\n",
    "\n",
    "#### 3.1.1 Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The first thing to be perfomed is to handle missing values indicated in paragraph `2.4.1`.\n",
    "\n",
    "For the attributes with very small number of missing values, namely, *job*, *marital*, *housing* and *loan*, the rows with the missing values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_cols = ['job', 'marital', 'housing', 'loan']\n",
    "\n",
    "df = df[~df[impute_cols].isin(['unknown']).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unknown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "For *education*, the missing values will be replaced with the most common value in the dataset for this variable. This approach is chosen because imputing with the mode preserves the existing distribution of categories without introducing bias from rare categories and it is effective with the small number of missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = df.loc[df['education'] != 'unknown', 'education'].mode()[0]\n",
    "df.loc[df['education'] == 'unknown', 'education'] = most_common\n",
    "print(f\"Imputed 'unknown' in '{'education'}' with: {most_common}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unknown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "The variable default contains a substantial number of missing values (approximately 20%). To address this, we apply a **logistic regression** model to impute the missing entries by predicting them based on the available data.\n",
    "\n",
    "Before modeling, the dataset must be properly encoded, following the steps described in Sections 3.4.1 and 3.4.2. To avoid modifying the original data during preprocessing, we work on a copy of the dataset. Once the missing values have been predicted, we use these results to update the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()\n",
    "missing_mask = df_temp['default'] == 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Binary encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categ_feat(dataset, cols):\n",
    "    for feature in cols:\n",
    "        print(feature)\n",
    "        print(dataset[feature].unique())\n",
    "        if dataset[feature].dropna().isin(['yes', 'no']).all():\n",
    "            dataset[feature] = (dataset[feature].values == 'yes').astype(int)\n",
    "            print(dataset[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_categ_feat(dataset=df_temp, cols=categFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Conversion of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.get_dummies(df_temp, drop_first=True, dtype=int)\n",
    "\n",
    "df_temp.rename({'default_yes': 'default'}, axis='columns', inplace = True)\n",
    "df_temp.drop(columns='default_unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc[:,'pdays'] = df_temp['pdays'].replace(999, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.insert(\n",
    "    loc = df_temp.columns.get_loc('pdays') + 1,  # insert after 'pdays'\n",
    "    column = 'never_contacted',\n",
    "    value = (df_temp['pdays'] == -1).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "The data has been cleaned, now we can perform logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_temp.loc[~missing_mask]\n",
    "impute_df = df_temp.loc[missing_mask]\n",
    "\n",
    "X_train = train_df.drop(columns=['default'])\n",
    "y_train = train_df['default']\n",
    "\n",
    "X_impute = impute_df.drop(columns=['default'])\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted_defaults = model.predict(X_impute)\n",
    "\n",
    "df.loc[missing_mask, 'default'] = predicted_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "#### 3.1.2 Duplicates Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Then we will delete duplicates to avoid redundancy of data, as indicated in paragraph `2.4.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate lines, if they exist\n",
    "\n",
    "shape_before = df.shape\n",
    "print('Shape before deleting duplicate values:',shape_before)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "shape_after = df.shape\n",
    "print('Shape after deleting duplicate values:',shape_after)\n",
    "\n",
    "percent = round((1-shape_after[0]/shape_before[0])*100,3)\n",
    "print(f\"Percentage of duplicates rows droped: {percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### 3.1.3 Outliers Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "To remove outliers identified in paragraph `2.4.2` we will use **Robust Scaler** during `Data Transformation` in paragraph `3.3`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### Data Exploration of cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object').columns:\n",
    "    print(f\"{col} unique values: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Based on the information about unique values there were no inconsistencies to be fixed, however we will change value '999' in *pdays* variable to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 999 with -1 to mark 'never contacted'\n",
    "df.loc[:,'pdays'] = df['pdays'].replace(999, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "Now we will redo the distribution analysis for categorical features and *pdays* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Almost every variable is dominated by one group. The loan status shows a strong imbalance with most of the clients not having a loan. Similarly, *default* is strongly dominated with 'no' with only few 'yes' entries. *job* variable is dominated by 'admin', education by 'university.degree', *contact* by 'cellular', *marital* by 'married' and *month* by 'may'. Only values of *housing* and *day_of_week* are distributed similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Filter out -1 (i.e., only keep rows where pdays != -1)\n",
    "plt.hist(df[df['pdays'] != -1]['pdays'], bins=30, edgecolor='black')\n",
    "\n",
    "plt.title('Histogram of pdays (Only Previously Contacted Clients)')\n",
    "plt.xlabel('Days Since Last Contact')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "We omitted '-1' values for the analysis purposes. \n",
    "*pdays* variable is not severly skewed but has two peaks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "### 3.2 Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, PlotCanvas = plt.subplots(nrows=math.ceil(len(categFeatures)/2), ncols=2, figsize=(16, 40))\n",
    "\n",
    "# Creating Grouped bar plots for each categorical predictor against the Target Variable \"class\"\n",
    "lin = 0\n",
    "for i, Categcol in enumerate(categFeatures):\n",
    "    col = i%2   \n",
    "    CrossTabResult=pd.crosstab(index=df[Categcol], columns=df['y'])\n",
    "    CrossTabResult.plot.bar(color=['green','red'], ax=PlotCanvas[lin,col])\n",
    "    if i%2 == 1:\n",
    "        lin = lin+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "These grouped bar charts display the frequency on the Y-axis and the category values on the X-axis. If the proportions of the target variable (e.g., 'yes' vs. 'no') are similar across all categories of a feature, it suggests that there is little to no relationship between that feature and the target.\n",
    "\n",
    "For example, if we look at a hypothetical plot like *day of week* vs. *y*, and observe that each day of a week has a similar 'yes' to 'no' ratio, it indicates that the day of week likely has no significant influence on *y*. In such cases, the feature and the target variable are likely not correlated. It can be also observed on the plot depicting *default* vs. *y*, *housing* vs. *y* and *loan* vs. *y*, which means that *default*, *housing* and *loan* are likely not correlated to *y*. \n",
    "\n",
    "However, there are a few variables that seems to be strongly correlated to *y*. Namely, *marital*, *poutcome*, *contact*, *job* and *education*. These variables do not maintain consistent proportions across the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(numericFeatures)\n",
    "cols = 3\n",
    "rows = math.ceil(num_features / cols)\n",
    "\n",
    "# histograms for continous data\n",
    "plt.figure(figsize=(5 * cols, 4 * rows))\n",
    "\n",
    "for i, feature in enumerate(nonDiscreteFeatures):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    if feature == 'pdays':\n",
    "        sns.histplot(data=df[df['pdays'] != -1], x='pdays', hue='y', kde=True, bins=30, palette={\"yes\": \"red\", \"no\": \"green\"})\n",
    "    else:\n",
    "        sns.histplot(df, x=feature, hue='y', kde=True, bins=30, palette={\"yes\": \"red\", \"no\": \"green\"})\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(discreteFeatures)\n",
    "\n",
    "# bar plot for discrete features\n",
    "\n",
    "nrows = math.ceil(len(discreteFeatures) / 2)\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 5))\n",
    "axes = axes.flatten()  # flatten for easy linear indexing\n",
    "\n",
    "for i, Categcol in enumerate(discreteFeatures):\n",
    "    ax = axes[i]\n",
    "    CrossTabResult = pd.crosstab(index=df[Categcol], columns=df['y'])\n",
    "    CrossTabResult.plot.bar(color=['green', 'red'], ax=ax)\n",
    "    ax.set_title(f'{Categcol} Distribution')\n",
    "    ax.set_xlabel(Categcol)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(len(discreteFeatures), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "Every variable except for *pdays* is dominated by 'no' class and *pdays* is dominated by 'yes' class, which means all of the variables are relevant for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### 3.4 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### 3.4.1 Encoding categorical variables\n",
    "\n",
    "To enable models to use the dataframe, data transformation, including encoding categorical variables must be conducted. Each variable with only two values ('yes' and 'no' in this dataframe) will be changed to '1' and '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = df.copy()\n",
    "\n",
    "engineerNumericFeatures = list(dfML.select_dtypes(include='number').columns)\n",
    "engineerCategFeatures = list(set(dfML.columns) - set(numericFeatures))\n",
    "\n",
    "encode_categ_feat(dataset=dfML, cols=engineerCategFeatures)\n",
    "\n",
    "dfML.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "#### 3.4.2 Feature Engineering\n",
    "\n",
    "We will perform feature engineering alongside data transformation, because we had to first encode the categorical values.\n",
    "\n",
    "There are a lot of categorical variables with multiple possible values. We will use `get_dummies()` function to one-hot encode them. This allows models that require numerical input to process categorical data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = pd.get_dummies(dfML, drop_first=True, dtype=int)\n",
    "\n",
    "dfML.rename({'y_yes': 'y'}, axis='columns', inplace = True)\n",
    "\n",
    "dfML.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "We will also create a new variable - *never_contacted* - related to the variables: *previous* and *pdays*. Value '1' will indicate that the client has never been called by another campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.insert(\n",
    "    loc = dfML.columns.get_loc('pdays') + 1,  # insert after 'pdays'\n",
    "    column = 'never_contacted',\n",
    "    value = (dfML['pdays'] == -1).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "#### 3.4.3 Handling variables with skewed distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Since not all of the skewed distrubutions concern variables with positive values we will use **Yeo-Johnson** scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float before transformation to avoid dtype issues\n",
    "cols_to_convert = skewed_cols + outliers_cols\n",
    "dfML = dfML.copy()\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    dfML[col] = dfML[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "print(df[skewed_cols].dtypes)\n",
    "\n",
    "dfML.loc[:, skewed_cols] = pt.fit_transform(dfML.loc[:, skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### 3.4.4 Handling outliers\n",
    "\n",
    "As indicated in paragraph `3.1`, we will use **Robust Scaler** to handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "print(outliers_cols)\n",
    "print(type(outliers_cols))\n",
    "\n",
    "# Fit the scaler on the numeric features and transform\n",
    "dfML.loc[:,outliers_cols] = scaler.fit_transform(dfML[outliers_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "#### 3.4.5 Dropping unnecessary variables\n",
    "\n",
    "We will drop *duration* variable since it highly affects the otput target, yet the value is not known before the class is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.drop(columns='duration', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### 3.5 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "To evaluate the models used in paragraph `4` we have to split dataset into a train and a test set. We will use an 80/20 split ratio to ensure sufficient amount of data for training while retaining enough samples for meaningful evaluation.\n",
    "Additionally, stratified sampling will applied to preserve the distribution of the target variable across both subsets, preventing bias during model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['y','duration'])\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Original dataframe\n",
    "# Combine features and target for training set\n",
    "train_df = X_train.copy()\n",
    "train_df['y'] = y_train\n",
    "\n",
    "# Combine features and target for testing set\n",
    "test_df = X_test.copy()\n",
    "test_df['y'] = y_test\n",
    "\n",
    "\n",
    "# This second dataframe is processed\n",
    "\n",
    "X = dfML.drop(columns=['y'])\n",
    "y = dfML['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "train_imbalance = X_train.copy()\n",
    "train_imbalance['y'] = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "To address class imbalance we will apply **SMOTETomek method**, a hybrid sampling technique that combines SMOTE (Synthetic Minority Over-sampling Technique) and Tomek links cleaning. This helps create a clearer separation between classes, which enables the model to learn more effectively and improve its ability to correctly identify minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfML = X_train_resampled.copy()\n",
    "train_dfML['y'] = y_train_resampled\n",
    "\n",
    "test_dfML = X_test.copy()\n",
    "test_dfML['y'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "### 3.5 Feature Selection\n",
    "\n",
    "By choosing only important features, model complexity is reduced and training efficiency is improved. To handle that we will use both filter and embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "#### 3.5.1 Filter Methods\n",
    "\n",
    "**Statistical Feature Selection**\n",
    "\n",
    "* Categorical vs Continuous---- ANOVA test\n",
    "* Categorical vs Categorical--- Chi-Square test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "**Categorical vs categorical using Chi-Square Test**\n",
    "\n",
    "Chi-Square test is conducted to check the correlation between two categorical variables\n",
    " - Assumption(H0): The two columns are NOT related to each other\n",
    " - Result of Chi-Sq Test: The Probability of H0 being True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):\n",
    "    # Creating an empty list of final selected predictors\n",
    "    FiltPredictors=[]\n",
    "\n",
    "    for predictor in CategoricalVariablesList:\n",
    "        CrossTabResult=pd.crosstab(index=inpData[TargetVariable], columns=inpData[predictor])\n",
    "        ChiSqResult = chi2_contingency(CrossTabResult)\n",
    "        \n",
    "        # If the ChiSq P-Value is <0.05, that means we reject H0\n",
    "        if (ChiSqResult[1] < 0.05):\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])\n",
    "        else:\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', ChiSqResult[1]) \n",
    "            FiltPredictors.append(predictor)\n",
    "            \n",
    "    return(FiltPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "newCategFeatures = []\n",
    "for categCol in categFeatures:\n",
    "    newCategFeatures.extend([col for col in dfML.columns if col.startswith(categCol)])\n",
    "\n",
    "print(numericFeatures)\n",
    "newNumericFeatures = numericFeatures\n",
    "newNumericFeatures.remove('duration')\n",
    "\n",
    "print('Categorical Features:', newCategFeatures)\n",
    "print('Numerical Features:', newNumericFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterCateg = FunctionChisq(inpData=train_dfML, TargetVariable='y', CategoricalVariablesList=newCategFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "The Chi-square test proves that *default_yes* is not correlated with y, which was shown on the plots (3.2). *education_illiterate* is also not correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "**Continuous vs categorical using ANOVA test**\n",
    " \n",
    "   - Assumption(H0): There is NO relation between the given variables (i.e. the average(mean) values of the numeric    predictor variable is same for all the groups in the categorical Target variable)\n",
    "\n",
    "ANOVA Test result: Probability of H0 being true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):\n",
    "    # Creating an empty list of final selected predictors\n",
    "    FiltPredictors=[]\n",
    "    \n",
    "    print('##### ANOVA Results ##### \\n')\n",
    "    for predictor in ContinuousPredictorList:\n",
    "        # Handle special cases for predictors (e.g., exclude -1 for pdays)\n",
    "        if predictor == 'pdays':\n",
    "            data = inpData[inpData[predictor] != -1]\n",
    "        else:\n",
    "            data = inpData\n",
    "        \n",
    "        # Group the data by the target variable and get lists of values per group\n",
    "        CategoryGroupLists = data.groupby(TargetVariable)[predictor].apply(list)\n",
    "        \n",
    "        # If any group is constant (only one unique value), f_oneway will raise warning or error\n",
    "        # So check for constant groups and skip ANOVA if found\n",
    "        if any([len(set(group)) <= 1 for group in CategoryGroupLists]):\n",
    "            print(f\"Skipping ANOVA for '{predictor}' because one or more groups have constant values.\")\n",
    "            FiltPredictors.append(predictor)  # Consider skipping or adding to filtered\n",
    "            continue\n",
    "        \n",
    "        AnovaResults = f_oneway(*CategoryGroupLists)\n",
    "        \n",
    "        # If the ANOVA P-Value is <0.05, that means we reject H0 (significant)\n",
    "        if AnovaResults[1] < 0.05:\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "        else:\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "            FiltPredictors.append(predictor)\n",
    "            \n",
    "    return FiltPredictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function to check which numeric variables are correlated with target\n",
    "# df_anova = train_dfML[train_dfML['pdays'] != -1] # remove -1s\n",
    "\n",
    "\n",
    "for col in newNumericFeatures:\n",
    "    grouped = dfML.groupby('y')[col]\n",
    "    for name, group in grouped:\n",
    "        if group.nunique() <= 1:\n",
    "            print(f\"Feature '{col}' is constant for target group '{name}'\")\n",
    "\n",
    "filterNumeric = FunctionAnova(inpData=train_dfML, TargetVariable='y', ContinuousPredictorList = newNumericFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterColumns = filterCateg + filterNumeric\n",
    "\n",
    "print(f\"Removed features by Filter methods: {FilterColumns}\")\n",
    "\n",
    "df1 = train_dfML.copy()\n",
    "df1_imbalance = train_imbalance.copy()\n",
    "\n",
    "print(df1.columns.tolist())\n",
    "\n",
    "df1.drop(columns=FilterColumns, axis=1, inplace=True)\n",
    "df1_imbalance.drop(columns=FilterColumns, axis=1, inplace=True)\n",
    "\n",
    "df1_test = test_dfML.copy()\n",
    "df1_test.drop(columns=FilterColumns, axis=1, inplace=True)\n",
    "\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "Filter methods removed two variables: *default_yes* and *education_illiterate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "#### 3.5.2 Embedded method - Lasso regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regularization(df):\n",
    "\n",
    "    X = df.iloc[:,:-1].copy()          \n",
    "    y = df.iloc[:,-1].copy() \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # fit a Logistic Regression model and feature selection altogether \n",
    "    # select the Lasso (l1) penalty.\n",
    "    # The selectFromModel class from sklearn, selects the features which coefficients are non-zero\n",
    "\n",
    "    sel_ = SelectFromModel(LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))\n",
    "\n",
    "    sel_.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "    # make a list with the selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    print(\"Number of features which coefficient was shrank to zero: \", np.sum(sel_.estimator_.coef_ == 0))\n",
    "    # identify the removed features like this:\n",
    "    removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "    print('Removed features by Lasso: ',removed_feats) \n",
    "\n",
    "    return X_train.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_SelectedColumns = lasso_regularization(train_dfML)\n",
    "\n",
    "Lasso_SelectedColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "Embedded method removed no variables, therefore we will not create a separate dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "With these methods, we prepared a dataframes for future testing and tuning to compare different models and select the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Modeling\n",
    "\n",
    "### 4.1 Evaluation metric selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "The ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = df1_test['y'].mean()\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "### 4.2 Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model = df1.drop(columns=['y'])\n",
    "y_train_model = df1['y']\n",
    "X_train_imb = df1_imbalance.drop(columns=['y'])\n",
    "y_train_imb = df1_imbalance['y']\n",
    "\n",
    "X_test_model = df1_test.drop(columns=['y'])\n",
    "y_test_model = df1_test['y']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_replace_result(model_name, new_result):\n",
    "    for i, r in enumerate(results):\n",
    "        if r['Model'] == model_name:\n",
    "            results[i] = new_result\n",
    "            return\n",
    "    results.append(new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model_name, y_pred, y_proba, train_time, pred_time):\n",
    "    # Evaluation (accuracy, recall, precision, AUC-PR, AUC-ROC, f1 score, confusion matrix)\n",
    "    accuracy = accuracy_score(y_test_model, y_pred)\n",
    "    recall = recall_score(y_test_model, y_pred)\n",
    "    precision = precision_score(y_test_model, y_pred)\n",
    "    f1 = f1_score(y_test_model, y_pred)\n",
    "    cm= confusion_matrix(y_test_model, y_pred)\n",
    "    prec, rec, threshold = precision_recall_curve(y_test_model, y_proba)\n",
    "    auc_pr = average_precision_score(y_test_model, y_proba)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test_model, y_proba)\n",
    "    auc_roc = roc_auc_score(y_test_model, y_proba)\n",
    "\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"F1 score: \", f1)\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Pred 0', 'Pred 1'],\n",
    "            yticklabels=['True 0', 'True 1'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Area Under the Receiver Operating Characteristic Curve (AUC-ROC): {auc_pr:.3f}\")\n",
    "    print(f\"Area Under Precision-Recall Curve (AUC-PR): {auc_pr:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC-ROC = {auc_roc:.3f}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(rec, prec, label=f\"AUC-PR = {auc_pr:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    new_record = {\n",
    "        'Model': model_name,\n",
    "        'Training time' : train_time,\n",
    "        'Prediction time' : pred_time,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC-ROC': auc_roc,\n",
    "        'AUC-PR': auc_pr\n",
    "    }\n",
    "\n",
    "    add_or_replace_result(model_name, new_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values: n_neighbors=5, metric='minkowski', p=2\n",
    "knn_basic = KNeighborsClassifier()\n",
    "\n",
    "start_train_knn = time.time()\n",
    "knn_basic.fit(X_train_scaled, y_train_model)\n",
    "train_time_knn= time.time() - start_train_knn\n",
    "\n",
    "start_pred_knn = time.time()\n",
    "pred_knn = knn_basic.predict(X_test_scaled)\n",
    "y_proba_knn = knn_basic.predict_proba(X_test_scaled)[:, 1]\n",
    "pred_time_knn = time.time() - start_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (accuracy, recall, precision, AUC-PR, f1 score, confusion matrix)\n",
    "evaluation('KNN', pred_knn, y_proba_knn, train_time_knn, pred_time_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values: penalty='l2', max_iter=100\n",
    "lr_basic = LogisticRegression(random_state=42)\n",
    "\n",
    "start_train_lr = time.time()\n",
    "lr_basic.fit(X_train_scaled, y_train_model)\n",
    "train_time_lr= time.time() - start_train_lr\n",
    "\n",
    "start_pred_lr = time.time()\n",
    "pred_lr = lr_basic.predict(X_test_scaled)\n",
    "y_proba_lr = lr_basic.predict_proba(X_test_scaled)[:, 1]\n",
    "pred_time_lr = time.time() - start_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('Logistic Regression', pred_lr, y_proba_lr, train_time_lr, pred_time_lr)\n",
    "coefficients = lr_basic.coef_[0]\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': np.abs(coefficients),\n",
    "    'Coefficient': coefficients\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values: criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1\n",
    "tree_basic = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "start_train_tree = time.time()\n",
    "tree_basic = tree_basic.fit(X_train_model, y_train_model)\n",
    "train_time_tree= time.time() - start_train_tree\n",
    "\n",
    "start_pred_tree = time.time()\n",
    "pred_tree = tree_basic.predict(X_test_model)\n",
    "y_proba_tree  = tree_basic.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_tree = time.time() - start_pred_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (accuracy, recall, precision, AUC-PR, f1 score, confusion matrix)\n",
    "evaluation('Decision Tree', pred_tree, y_proba_tree, train_time_tree, pred_time_tree)\n",
    "feature_names = X_train_model.columns\n",
    "importances = tree_basic.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_basic = GaussianNB()\n",
    "\n",
    "start_train_nb = time.time()\n",
    "nb_basic.fit(X_train_model, y_train_model)\n",
    "train_time_nb= time.time() - start_train_nb\n",
    "\n",
    "start_pred_nb = time.time()\n",
    "pred_nb = nb_basic.predict(X_test_model)\n",
    "y_proba_nb  = nb_basic.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_nb = time.time() - start_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('Naive Bayes', pred_nb, y_proba_nb, train_time_nb, pred_time_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Default values: C=1.0, kernel='rbf', gamma='scale'\n",
    "svm_basic = SVC(probability=True, random_state=42)\n",
    "\n",
    "start_train_svm = time.time()\n",
    "svm_basic.fit(X_train_scaled, y_train_model)\n",
    "train_time_svm = time.time() - start_train_svm\n",
    "\n",
    "start_pred_svm = time.time()\n",
    "pred_svm= svm_basic.predict(X_test_scaled)\n",
    "y_proba_svm = svm_basic.predict_proba(X_test_scaled)[:, 1]\n",
    "pred_time_svm = time.time() - start_pred_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('Support Vector Machines', pred_svm, y_proba_svm, train_time_svm, pred_time_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "#### 4.3 Baseline models comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "if not results_df.empty:\n",
    "    results_df = results_df.sort_values(by='AUC-PR', ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-PR']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Model Performance Comparison')\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.barplot(data=results_df, x='Model', y=metric)\n",
    "    plt.title(metric)\n",
    "    plt.ylabel('')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "### 4.4 Hyperparameter Tuning for Baseline models\n",
    "- TODO: Define hyperparameter search space\n",
    "- TODO: Apply Grid Search or Random Search\n",
    "- TODO: Use cross-validation for tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_basic.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lr = {\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': np.logspace(-3, 3, 10),\n",
    "    'model__solver': ['saga'],\n",
    "    'model__max_iter': [5000, 10000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTETomek(random_state=42)),\n",
    "    ('model', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "lr_rs = RandomizedSearchCV(pipe_lr, param_lr, n_iter=20, cv=5, scoring='average_precision', random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time_lr_rs = time.time()\n",
    "lr_rs.fit(X_train_imb, y_train_imb)\n",
    "train_time_lr_rs = time.time() - start_time_lr_rs\n",
    "\n",
    "best_lr = lr_rs.best_estimator_\n",
    "print(\"Best params:\", lr_rs.best_params_)\n",
    "print(\"Best AUC-PR:\", lr_rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred_lr_rs = time.time()\n",
    "y_pred_best_lr = best_lr.predict(X_test_model)\n",
    "y_proba_best_lr = best_lr.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_lr_rs = time.time() - start_pred_lr_rs\n",
    "\n",
    "evaluation(\"Logistic Regression (AUC-PR tuned)\", y_pred_best_lr, y_proba_best_lr, train_time_lr_rs, pred_time_lr_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_basic.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_knn = {\n",
    "    'model__n_neighbors': [3, 5, 10, 15, 20, 50],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__metric': ['euclidean', 'manhattan']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe =Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTETomek(random_state=42)),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn_gs = GridSearchCV(knn_pipe, param_knn, cv=5,  scoring='average_precision')\n",
    "start_time_knn_gs = time.time()\n",
    "knn_gs_fit = knn_gs.fit(X_train_imb, y_train_imb)\n",
    "train_time_knn_gs = time.time() - start_time_knn_gs\n",
    "knn_best = knn_gs.best_estimator_\n",
    "\n",
    "print(\"The best parameters:\", knn_gs.best_params_)\n",
    "print(f\"The best AUC-PR: {knn_gs.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred_knn_gs = time.time()\n",
    "y_pred_best = knn_best.predict(X_test_model)\n",
    "y_proba_best = knn_best.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_knn_gs = time.time() - start_pred_knn_gs\n",
    "\n",
    "evaluation(\"KNN (AUC-PR tuned)\", y_pred_best, y_proba_best, train_time_knn_gs, pred_time_knn_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "#### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree_basic.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tree= {\n",
    "    'model__max_depth': randint(1, 20),\n",
    "    'model__min_samples_split': randint(2, 20),\n",
    "    'model__min_samples_leaf': randint(1, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe = Pipeline([\n",
    "    ('smote', SMOTETomek(random_state=42)),\n",
    "    ('model', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "tree_rs = RandomizedSearchCV(tree_pipe, param_tree, n_iter=20, cv=5, scoring='average_precision', random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time_tree_rs = time.time()\n",
    "tree_rs.fit(X_train_imb, y_train_imb)\n",
    "train_time_tree_rs = time.time() - start_time_tree_rs\n",
    "\n",
    "best_tree = tree_rs.best_estimator_\n",
    "print(\"Best params:\", tree_rs.best_params_)\n",
    "print(\"Best AUC-PR:\", tree_rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred_tree_rs = time.time()\n",
    "y_pred_best_tree = best_tree.predict(X_test_model)\n",
    "y_proba_best_tree = best_tree.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_tree_rs = time.time() - start_pred_tree_rs\n",
    "\n",
    "evaluation(\"Decision Tree (AUC-PR tuned)\", y_pred_best_tree, y_proba_best_tree, train_time_tree_rs, pred_time_tree_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "### 4.5 Ensemble Methods\n",
    "- TODO: Create ensemble models (voting, stacking, blending)\n",
    "- TODO: Combine best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble models\n",
    "# TODO: Implement ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values: n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1\n",
    "forest_basic = RandomForestClassifier(random_state=42)\n",
    "\n",
    "start_train_forest = time.time()\n",
    "forest_basic.fit(X_train_model, y_train_model)\n",
    "train_time_forest= time.time() - start_train_forest\n",
    "\n",
    "start_pred_forest = time.time()\n",
    "pred_forest = forest_basic.predict(X_test_model)\n",
    "y_proba_forest = forest_basic.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_forest = time.time() - start_pred_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(\"Random Forest\", pred_forest, y_proba_forest, train_time_forest, pred_time_forest)\n",
    "feature_names = X_train_model.columns\n",
    "importances = forest_basic.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values: learning_rate=0.1, n_estimators=100, subsample=1.0, max_depth=3, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1\n",
    "gb_basic = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "start_train_gb = time.time()\n",
    "gb_basic.fit(X_train_model, y_train_model)\n",
    "train_time_gb= time.time() - start_train_gb\n",
    "\n",
    "start_pred_gb = time.time()\n",
    "pred_gb = gb_basic.predict(X_test_model)\n",
    "y_proba_gb = gb_basic.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_gb = time.time() - start_pred_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(\"Gradient Boosting\", pred_gb, y_proba_gb, train_time_gb, pred_time_gb)\n",
    "feature_names = X_train_model.columns\n",
    "importances = gb_basic.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training the Bagging Classifier...\")\n",
    "start_train_bagging = time.time()\n",
    "bagging_model.fit(X_train_model, y_train_model)\n",
    "train_time_bagging= time.time() - start_train_bagging\n",
    "\n",
    "\n",
    "start_pred_bagging = time.time()\n",
    "y_pred_bagging = bagging_model.predict(X_test_model)\n",
    "y_proba_bagging  = bagging_model.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_bagging = time.time() - start_pred_bagging\n",
    "\n",
    "accuracy = accuracy_score(y_test_model, y_pred_bagging)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('Bagging Classifier', y_pred_bagging, y_proba_bagging, train_time_bagging, pred_time_bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'estimator__max_depth': [1, 2]\n",
    "}\n",
    "\n",
    "weak_learner = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=weak_learner,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=adaboost_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='average_precision',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for AdaBoostClassifier...\")\n",
    "start_train_grid = time.time()\n",
    "grid_search.fit(X_train_model, y_train_model)\n",
    "train_time_grid = time.time() - start_train_grid\n",
    "print(f\"GridSearchCV training completed in {train_time_grid:.4f} seconds.\")\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_adaboost_model = grid_search.best_estimator_\n",
    "\n",
    "start_pred_adaboost = time.time()\n",
    "y_pred_adaboost = best_adaboost_model.predict(X_test_model)\n",
    "y_proba_adaboost = best_adaboost_model.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_adaboost = time.time() - start_pred_adaboost\n",
    "\n",
    "print(f\"Prediction with the best model completed in {pred_time_adaboost:.4f} seconds.\")\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test_model, y_pred_adaboost)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# best recall parameters:\n",
    "# param_best_recall = {\n",
    "#     'n_estimators': [50],\n",
    "#     'learning_rate': [1.0],\n",
    "#     'estimator__max_depth': [1]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('AdaBoost', y_pred_adaboost, y_proba_adaboost, train_time_grid, pred_time_adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "count_neg = np.sum(y_train_model == 0)\n",
    "count_pos = np.sum(y_train_model == 1)\n",
    "scale_pos_weight = count_neg / count_pos\n",
    "\n",
    "print(f\"Calculated scale_pos_weight for imbalance: {scale_pos_weight:.2f}\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training the XGBoost Classifier...\")\n",
    "start_train_xgb = time.time()\n",
    "xgb_model.fit(X_train_model, y_train_model)\n",
    "train_time_xgb = time.time() - start_train_xgb\n",
    "\n",
    "start_pred_xgb = time.time()\n",
    "y_pred_xgb = xgb_model.predict(X_test_model)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_xgb = time.time() - start_pred_xgb\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_xgb:.4f} seconds.\")\n",
    "print(f\"Prediction completed in {pred_time_xgb:.4f} seconds.\")\n",
    "\n",
    "accuracy = accuracy_score(y_test_model, y_pred_xgb)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('XGBoost', y_pred_xgb, y_proba_xgb, train_time_xgb, pred_time_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# --- Create the LightGBM Classifier with balanced class weights ---\n",
    "# No need to manually calculate scale_pos_weight anymore.\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    class_weight='balanced', # Handles class imbalance automatically\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Train the model and time the process ---\n",
    "print(\"Training the LightGBM Classifier with class_weight='balanced'...\")\n",
    "start_train_lgbm = time.time()\n",
    "lgbm_model.fit(X_train_model, y_train_model)\n",
    "train_time_lgbm = time.time() - start_train_lgbm\n",
    "\n",
    "# --- Make predictions and time the process ---\n",
    "start_pred_lgbm = time.time()\n",
    "y_pred_lgbm = lgbm_model.predict(X_test_model)\n",
    "y_proba_lgbm = lgbm_model.predict_proba(X_test_model)[:, 1]\n",
    "pred_time_lgbm = time.time() - start_pred_lgbm\n",
    "\n",
    "# --- Print the timing results ---\n",
    "print(f\"\\nTraining completed in {train_time_lgbm:.4f} seconds.\")\n",
    "print(f\"Prediction completed in {pred_time_lgbm:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation('LightGBM', y_pred_lgbm, y_proba_lgbm, train_time_lgbm, pred_time_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Neural Networks\n",
    "### 5.1 Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "CALCULATE = False\n",
    "\n",
    "dfNN = df1.copy()\n",
    "\n",
    "# Create a MLP network\n",
    "X = dfNN.drop(columns=['y'])\n",
    "y = dfNN['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.05],\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 30), (100, 50)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'batch_size': [32, 64, 'auto'],\n",
    "    'max_iter': [500, 1000, 1500],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.0001, 0.01],\n",
    "}\n",
    "\n",
    "if CALCULATE:\n",
    "    grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, n_jobs=-1, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "    print(\"Starting Grid Search...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "    y_pred_NN = best_mlp.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_NN)\n",
    "    print(f\"\\nOptimized Neural Network Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "else:\n",
    "    best_params_from_grid = {\n",
    "        'alpha': 0.001,\n",
    "        'hidden_layer_sizes': (100, 50),\n",
    "        'solver': 'adam',\n",
    "        'batch_size': 64,\n",
    "        'max_iter': 500,\n",
    "        'activation': 'tanh',\n",
    "        'learning_rate_init': 0.0001,\n",
    "    }\n",
    "\n",
    "    mlp.set_params(**best_params_from_grid)\n",
    "\n",
    "    print(\"Starting Best Params NN...\")\n",
    "    start_train_NN = time.time()\n",
    "    #mlp.fit(X_train, y_train)\n",
    "    mlp.fit(X_train_model, y_train_model)\n",
    "    train_time_NN= time.time() - start_train_NN\n",
    "\n",
    "    print(\"\\nBest parameters:\")\n",
    "    print(best_params_from_grid)\n",
    "\n",
    "    start_pred_NN = time.time()\n",
    "    y_pred_NN = mlp.predict(X_test_model)\n",
    "    y_proba_NN = mlp.predict_proba(X_test_model)[:, 1]\n",
    "    pred_time_NN = time.time() - start_pred_NN\n",
    "    accuracy = accuracy_score(y_test_model, y_pred_NN)\n",
    "    print(f\"\\nOptimized Neural Network Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE:\n",
    "    best_params = grid_search.best_params_\n",
    "else:\n",
    "    best_params = best_params_from_grid\n",
    "\n",
    "best_mlp = MLPClassifier(**best_params,\n",
    "                         random_state=42,\n",
    "                         early_stopping=True,\n",
    "                         verbose=False)\n",
    "\n",
    "print(\"\\nRetraining the best model with early stopping to plot history...\")\n",
    "best_mlp.fit(X_train_model, y_train_model)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot training loss\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color=color)\n",
    "ax1.plot(best_mlp.loss_curve_, color=color, label='Training Loss')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for validation accuracy\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Validation Accuracy', color=color)\n",
    "ax2.plot(best_mlp.validation_scores_, color=color, label='Validation Accuracy')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Training Loss vs. Validation Accuracy')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#TODO zamieni precision na recall oraz mona dodatkowo doda f1-score poniewa jest to balans pomiedzy precision i recall\n",
    "#TODO look at validation loss for overfitting!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(\"NeuralNetwork\", y_pred_NN, y_proba_NN, train_time_NN, pred_time_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation\n",
    "\n",
    "### 5.1 Model Performance Metrics\n",
    "- TODO: Calculate accuracy, precision, recall, F1-score\n",
    "- TODO: Generate ROC curves and calculate AUC\n",
    "- TODO: Create confusion matrices\n",
    "- TODO: Calculate business-relevant metrics (cost/benefit analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "# TODO: Calculate and compare all metrics across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "# TODO: Create ROC curves, precision-recall curves\n",
    "# TODO: Create confusion matrices\n",
    "# TODO: Create comparison charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {},
   "source": [
    "### 5.2 Model Interpretation\n",
    "- TODO: Analyze feature importance\n",
    "- TODO: Interpret model predictions\n",
    "- TODO: Validate model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# TODO: Extract and visualize feature importance from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation\n",
    "# TODO: Use SHAP, LIME, or other interpretation methods if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195",
   "metadata": {},
   "source": [
    "### 5.3 Model Validation\n",
    "- TODO: Perform cross-validation\n",
    "- TODO: Test on holdout set\n",
    "- TODO: Check for overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "# TODO: Perform k-fold cross-validation on best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation on test set\n",
    "# TODO: Evaluate final model(s) on unseen test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "### 5.4 Business Impact Assessment\n",
    "- TODO: Translate model performance to business value\n",
    "- TODO: Calculate expected ROI or cost savings\n",
    "- TODO: Provide actionable recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis\n",
    "# TODO: Calculate business metrics (conversion rate improvement, cost reduction, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Conclusions and Recommendations\n",
    "\n",
    "### 6.1 Summary of Findings\n",
    "- TODO: Summarize key insights from data exploration\n",
    "- TODO: Summarize model performance\n",
    "- TODO: Identify most important predictive features\n",
    "\n",
    "### 6.2 Best Model Selection\n",
    "- TODO: Select and justify the best model\n",
    "- TODO: Document model strengths and limitations\n",
    "\n",
    "### 6.3 Recommendations\n",
    "- TODO: Provide actionable business recommendations\n",
    "- TODO: Suggest customer prioritization strategy\n",
    "- TODO: Recommend campaign optimization strategies\n",
    "\n",
    "### 6.4 Future Work\n",
    "- TODO: Suggest model improvements\n",
    "- TODO: Identify additional data needs\n",
    "- TODO: Propose deployment strategy\n",
    "\n",
    "### 6.5 Lessons Learned\n",
    "- TODO: Document challenges faced\n",
    "- TODO: Share insights from the project\n",
    "- TODO: Note what would be done differently\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "## Project Notes and Team Collaboration\n",
    "\n",
    "### Team Members\n",
    "- TODO: List team members and responsibilities\n",
    "\n",
    "Julia Kardasz 1250264\n",
    "\n",
    "### Project Timeline\n",
    "- TODO: Document project milestones and deadlines\n",
    "\n",
    "### References\n",
    "- TODO: Add references to papers, documentation, and resources used\n",
    "\n",
    "---\n",
    "*This notebook follows the CRISP-DM methodology for data mining projects*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
