{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#  Project\n",
    "\n",
    "**Project Goal:** \n",
    "\n",
    "**Dataset Period:**\n",
    "\n",
    "**Methodology:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Business Objectives\n",
    "TODO\n",
    "\n",
    "### 1.2 Project Goals and Successs Criteria\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "\n",
    "import math\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Understanding\n",
    "\n",
    "### 2.1 Data Collection\n",
    "\n",
    "The first step is to load the datasets into the working environment. This involves importing the necessary libraries and reading the data file into a suitable data structure, namely, a DataFrame using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "weather = pd.read_csv(\"weather.csv\", parse_dates=[\"time\"])\n",
    "energy = pd.read_csv(\"energy_dataset.csv\", parse_dates=[\"time\"])\n",
    "\n",
    "# Set 'time' as index\n",
    "weather = weather.set_index(\"time\")\n",
    "energy = energy.set_index(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.2 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### Basic dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The *energy* dataset contains 35064 entries and 24 features, representing hourly records of electricity generation from various sources, total system load, and the day-ahead market price. Each row corresponds to one hour of energy system operation, and the goal is to forecast the electricity price one hour and one day ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The *weather* dataset also contains 35064 entries and 5 features representing hourly meteorological measurements such as temperature, pressure, humidity, and wind speed. Each row corresponds to one hour of weather conditions, and these variables are used as exogenous inputs to improve electricity price forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 2.3 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### 2.3.1 Target variable analysis\n",
    "\n",
    "The target variable `price_day_ahead` represents the eletricity market price for the upcoming hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col='price_day_ahead'\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(energy[target_col], bins=50, edgecolor='black')\n",
    "plt.title(\"Histogram of Day-Ahead Electricity Price\")\n",
    "plt.xlabel(\"Price (€/MWh)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The histogram shows a distribution that is very close to a normal distribution, although it is slightly right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(energy[target_col])\n",
    "plt.title(\"Time Series of Electricity Price (Day-Ahead)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price (€/MWh)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The time series plot shows strong short-term fluctuations and clear seasonal patterns, with occasional price spikes. Prices vary over time, indicating non-stationarity and the presence of both volatility and periodic behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### 2.3.2 Feature distribution analysis\n",
    "\n",
    "Now we will perform feature distribution analysis to examine how the data values are spread across the datasets. We will use plots and histograms to visualise the distributions features. Box plots for will be skipped in this step, as they will be specifically used for outlier detection in paragraph `2.4.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_distribution(nonDiscreteFeatures, df):\n",
    "    nrows = math.ceil(len(nonDiscreteFeatures) / 2)\n",
    "    ncols = 2\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(nonDiscreteFeatures):\n",
    "        df[col].hist(bins=30, ax=axes[i])\n",
    "        axes[i].set_title(f'{col} Distribution')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "All of the features are numerical and continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##### Energy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_features = [col for col in energy.columns \n",
    "                   if col not in ['time', 'price_day_ahead']]\n",
    "feat_distribution(energy_features, energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "None of the features has a normal distribution. `generation_hydro_pumped_storage_consumption`, `generation_hydro_water_reservoir`, and `generation_solar` are strongly right-skewed, while `generation_wind_onshore` is moderately right-skewed. In contrast, `generation_waste` is left-skewed.\n",
    "Variables with skewed distributions will be handled in paragraph 3.4.3, because skewness can negatively affect the models used later in paragraph 4.2, particularly KNN, SVR, and Linear Regression, which rely on distance metrics or assumptions of linearity.\n",
    "\n",
    "`generation_marine`, `generation_geothermal`, `generation_fossil_oil_shale`, `generation_fossil_peat`, and `generation_fossil_coal-derived_gas` contain only zeros, which is consistent with the dataset description provided in paragraph 2.2.\n",
    "\n",
    "The data represented by the remaining variables generally show irregular, multimodal, or heavily skewed shapes, reflecting the diverse and highly variable nature of electricity generation across different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##### Weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features = [col for col in weather.columns if col != 'time']\n",
    "feat_distribution(weather_features, weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "`temperature` and `pressure` distribution is close to normal, whereas `humidity` is slightly left-skewed and `wind_speed` is right-skewed.\n",
    "\n",
    "Skeweness will be handled in paragraph DOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_cols = ['generation_hydro_pumped_storage_consumption','generation_hydro_water_reservoir','generation_solar','generation_wind_onshore','generation_waste','wind_speed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 2.4 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### 2.4.1 Identify missing values\n",
    "\n",
    "In the first place, we will check for missing values to ensure data completeness and avoid potential issues during analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % missing values by column\n",
    "\n",
    "def missing_values_info(df):\n",
    "    nulls = df.isnull().sum()\n",
    "    percent = round(nulls / df.shape[0] * 100, 3)\n",
    "    \n",
    "    nullvalues = pd.concat([nulls, percent], axis=1)\n",
    "    nullvalues.columns = [\"Count\", \"%\"]\n",
    "    \n",
    "    return nullvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_info(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "The proportion of missing values varies across the energy-generation features. Most variables contain only a very small fraction of missing entries (around 0.05% each), including `generation_biomass`, `generation_fossil_gas`, `generation_solar`, `generation_wind_onshore`, and many others.\n",
    "A few features have slightly higher but still low missing rates, such as `generation_nuclear` at 0.048% and `total_load_actual` at 0.103%.\n",
    "\n",
    "Two variables — `generation_hydro_pumped_storage_aggregated` and `forecast_wind_offshore_eday_ahead` — have 100% missing values, meaning they contain no usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping the NaNs\n",
    "energy_features = [\n",
    "    f for f in energy_features\n",
    "    if energy[f].notna().sum() > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_info(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "There are no missing values in `weather` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "#### 2.4.2 Identify outliers\n",
    "\n",
    "Based on dataframes information in paragraph `2.2` there might be potential outliers such as values at the extreme ends of the distributions (e.g., very high generation levels or unusually low/high prices). These points can disproportionately influence analysis and model results. That's why now we will identify outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "DOK \n",
    "Rolling Window Analysis: calculate rolling mean and standard deviation. Values\n",
    "that deviate significantly from the rolling mean (e.g., beyond ±3 standard\n",
    "deviations) may be considered outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "##### 2.4.2.1 Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.boxplot(\n",
    "    energy[target_col],\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "    medianprops=dict(color='red'),\n",
    "    whiskerprops=dict(color='blue'),\n",
    "    capprops=dict(color='blue'),\n",
    "    flierprops=dict(marker='o', markersize=4, markerfacecolor='blue')\n",
    ")\n",
    "plt.title(\"Boxplot of Day-Ahead Electricity Price\")\n",
    "plt.ylabel(\"Price (€/MWh)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "The boxplot shows clear price spikes, confirming the presence of outliers in the target variable. These extreme values reflect real market volatility but may negatively affect several forecasting models. Therefore, while they are kept in the dataset, they will be handled later in paragraph `3.4.3` DOK through appropriate preprocessing to minimise their impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "##### 2.4.2.2 Energy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(numeric_features, df):\n",
    "    num_plots = len(numeric_features)\n",
    "    cols = 2\n",
    "    rows = math.ceil(num_plots / cols)\n",
    "\n",
    "    plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        sns.boxplot(y=df[feature])\n",
    "        plt.title(f\"Boxplot: {feature}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def outliers_detection(numericFeatures, df):\n",
    "    Q1 = df[numericFeatures].quantile(0.25)\n",
    "    Q3 = df[numericFeatures].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outliers = ((df[numericFeatures] < (Q1 - 1.5 * IQR)) | (df[numericFeatures] > (Q3 + 1.5 * IQR)))\n",
    "    print(\"Number of outliers per numeric feature:\")\n",
    "    print(outliers.sum())\n",
    "\n",
    "    outliers_cols = outliers.any()\n",
    "    outliers_cols = outliers_cols[outliers_cols].index.tolist()\n",
    "    print(\"\\nColumns containing outliers:\")\n",
    "    print(outliers_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots(energy_features, energy)\n",
    "outliers_detection(energy_features, energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We performed outliers detection on all numerical features using the Interquartile Range (IQR) method and visualized it using boxplots.\n",
    "\n",
    "Features such as `generation_fossil_gas`, `generation_hydro_pumped_storage_consumption` and `generation_other` contain a high number of outlier values.\n",
    "`generation_wind_onshore`, `generation_waste`,`generation_hydro_water_reservoir` and `generation_fossil_oil` also contained a noticeable number out outliers.\n",
    "`generation_biomass` contains a few outliers.\n",
    "The rest of the features of `energy` dataset showed no outliers according to the IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "##### 2.4.2.3 Weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots(weather_features, weather)\n",
    "outliers_detection(weather_features, weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "`pressure` and `wind_speed` contain a noticeable number of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_cols = ['pressure', \n",
    "                 'wind_speed', \n",
    "                 'generation_fossil_gas', \n",
    "                 'generation_hydro_pumped_storage_consumption', \n",
    "                 'generation_other', \n",
    "                 'generation_wind_onshore', \n",
    "                 'generation_waste',\n",
    "                 'generation_hydro_water_reservoir',\n",
    "                 'generation_biomass',\n",
    "                 'generation_fossil_oil' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### 2.4.3 Check data duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.index.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "There are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Data Cleaning\n",
    "\n",
    "#### 3.1.1 Missing values\n",
    "\n",
    "The first thing to be perfomed is to handle missing values indicated in paragraph `2.4.1`.\n",
    "\n",
    "The missing values cannot be romoved that's why the NaNs will be filled in and columns only with NaNs will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remaining NaNs in energy:\", energy.isna().sum().sum())\n",
    "\n",
    "# Drop columns that contain only NaN values\n",
    "energy = energy.dropna(axis=1, how='all')\n",
    "\n",
    "# Interpolate missing values (time-series aware)\n",
    "energy.index = pd.to_datetime(energy.index, errors='coerce', utc=True)\n",
    "energy = energy.interpolate(method='time')\n",
    "\n",
    "# Forward-fill remaining NaNs (edge cases)\n",
    "energy = energy.ffill().bfill()\n",
    "\n",
    "print(\"Remaining NaNs in energy:\", energy.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### 3.1.2 Handling outliers\n",
    "\n",
    "To remove outliers identified in paragraph `2.4.2` we will perform winsorization and use **Robust Scaler** during `Data Transformation` in paragraph `3.3`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### 3.2 Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Here we will skip features that consist only of values 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = energy.join(weather, how=\"inner\")\n",
    "\n",
    "# remove columns that are entirely zero\n",
    "non_zero_cols = df_analysis.columns[(df_analysis != 0).any()]\n",
    "\n",
    "# keep only numeric features\n",
    "num_df = df_analysis[non_zero_cols].select_dtypes(include=\"number\")\n",
    "\n",
    "# compute correlation with price\n",
    "corr = num_df.corr()[target_col].drop(labels=[target_col]).sort_values()\n",
    "\n",
    "plt.figure(figsize=(8,12))\n",
    "corr.plot(kind=\"barh\")\n",
    "plt.title(\"Correlation with Day-Ahead Price\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "The diagram shows that some variables are correlated positively, some nagetively and some are only slightly or not correlated with the target variable at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "#### Correlation between the eletricity price and the day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(energy.index))\n",
    "# print(energy.index[:5])\n",
    "\n",
    "weekday_price = energy['price_day_ahead'].groupby(energy.index.to_series().dt.dayofweek).mean()\n",
    "print(weekday_price)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "weekday_price.plot(kind='bar')\n",
    "plt.title(\"Average Day-Ahead Electricity Price by Day of the Week\")\n",
    "plt.ylabel(\"Price (€/MWh)\")\n",
    "plt.xlabel(\"Day of Week (0=Mon, 6=Sun)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "There's no strong dependency between the day of the week and the eletricity price, however the price tends to be lower on the weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "##### Correlation between the temperature and the eletricity price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_analysis[\"temperature\"]\n",
    "y = df_analysis[target_col]\n",
    "\n",
    "# smooth curve\n",
    "low = lowess(y, x, frac=0.15)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x, y, s=5, alpha=0.1)\n",
    "plt.plot(low[:,0], low[:,1], linewidth=3)\n",
    "\n",
    "plt.title(\"LOWESS Smoothed Relationship: Temperature vs Price\")\n",
    "plt.xlabel(\"Temperature (K)\")\n",
    "plt.ylabel(\"Day-Ahead Price (€/MWh)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "There's no significant correlation between the eletricity price and the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### 3.3 Data Transformation\n",
    "\n",
    "Since all the features are numerical they don't have to be encoded in any way.\n",
    "\n",
    "#### 3.3.1 Dropping unnecessary variables\n",
    "\n",
    "There are variables with all 0 values. They will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = energy.join(weather, how=\"inner\")\n",
    "\n",
    "dfML = df.copy()\n",
    "\n",
    "zero_cols = dfML.columns[(dfML == 0).all()]\n",
    "print(\"Dropped columns with all zeros:\", list(zero_cols))\n",
    "\n",
    "dfML = dfML.drop(columns=zero_cols)\n",
    "\n",
    "dfML.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.3.1 Handling variables with skewed distribution\n",
    "\n",
    "Since not all of the skewed distrubutions concern variables with positive values we will use **Yeo-Johnson** scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float before transformation to avoid dtype issues\n",
    "dfML = dfML.copy()\n",
    "\n",
    "for col in skewed_cols:\n",
    "    dfML[col] = dfML[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "print(df[skewed_cols].dtypes)\n",
    "\n",
    "dfML.loc[:, skewed_cols] = pt.fit_transform(dfML.loc[:, skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "#### 3.3.2 Handling outliers\n",
    "\n",
    "As indicated in paragraph `3.1`, we will use **Robust Scaler** to handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "print(outliers_cols)\n",
    "print(type(outliers_cols))\n",
    "\n",
    "# Fit the scaler on the numeric features and transform\n",
    "dfML.loc[:,outliers_cols] = scaler.fit_transform(dfML[outliers_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### 3.4 Data Splitting\n",
    "\n",
    "To evaluate the models used in paragraph `4` we have to split dataset into a train and a test set. We will use an 75/25 split ratio (the whole dataset represents 4 years of data and the test set will consist of last 12 months) to ensure sufficient amount of data for training while retaining enough samples for meaningful evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 12 months for test\n",
    "split_date = dfML.index.max() - pd.DateOffset(years=1)\n",
    "\n",
    "train_dfML = dfML[dfML.index <= split_date]\n",
    "test_dfML  = dfML[dfML.index > split_date]\n",
    "\n",
    "print(\"Split date:\", split_date)\n",
    "print(\"Train:\", train_dfML.shape)\n",
    "print(\"Test:\", test_dfML.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### 3.5 Feature selection\n",
    "\n",
    "#### Embedded method - Lasso regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regularization(df, target_col=\"price_day_ahead\"):\n",
    "\n",
    "    X = df.drop(columns=[target_col]).copy()\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    lasso = Lasso(alpha=0.01, random_state=42, max_iter=5000)\n",
    "\n",
    "    sel_ = SelectFromModel(lasso)\n",
    "    sel_.fit(X_scaled, y)\n",
    "\n",
    "    selected_feat = X.columns[sel_.get_support()]\n",
    "    removed_feat = X.columns[~sel_.get_support()]\n",
    "\n",
    "    print(f\"Selected features ({len(selected_feat)}):\")\n",
    "    print(selected_feat.tolist())\n",
    "\n",
    "    print(f\"\\nRemoved features ({len(removed_feat)}):\")\n",
    "    print(removed_feat.tolist())\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_SelectedColumns = lasso_regularization(train_dfML)\n",
    "\n",
    "Lasso_SelectedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfML = train_dfML[list(Lasso_SelectedColumns) + [target_col]]\n",
    "test_dfML  = test_dfML[list(Lasso_SelectedColumns) + [target_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "With these methods, we prepared a dataframes for future testing and tuning to compare different models and select the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Modelling\n",
    "\n",
    "### 4.1 Statistical Models\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### 4.2 Machine Learning Models\n",
    "\n",
    "comment\n",
    "\n",
    "#### 4.2.1 Build supervised sliding-window datasets\n",
    "\n",
    "We will be using two functions two build sliding-window datasets. `make_sliding_window` is used for single-step predicitons and `slideWindow` for multi-step predictions. Later on these functions will be used to build the sliding-window datasets after choosing the best window size in paragraph `4.2.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sliding_window(series, window_size, horizon=1): # for single-step\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size - horizon + 1):\n",
    "        X.append(series[i:i + window_size])\n",
    "        y.append(series[i + window_size + horizon - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def slideWindow(time_series, window_in, horizon): # for multi-step\n",
    "    if (len(time_series) - window_in - horizon + 1) <= 0:\n",
    "       raise ValueError(\"Time series too short for window size or horizon.\")\n",
    "    \n",
    "    d = time_series.values\n",
    "    X, y = [], []\n",
    "\n",
    "    for start in range(len(time_series)-window_in):\n",
    "        end = start + window_in\n",
    "        out = end + horizon\n",
    "        X.append(d[start:end].reshape(-1))\n",
    "        y.append(d[end:out].ravel()) # ravel() is equivalent to reshape(-1): returnscontiguous flattened array\n",
    "        cols_x = [f'x{i}' for i in range(1, window_in+1)]\n",
    "        cols_y = [f'y{i}' for i in range(1, horizon+1)]\n",
    "        df_X = pd.DataFrame(X, columns=cols_x)\n",
    "        df_y = pd.DataFrame(y, columns=cols_y)\n",
    "\n",
    "    return pd.concat([df_X, df_y], axis=1).dropna()\n",
    "\n",
    "def slideWindow2(time_series, window_in, horizon):\n",
    "    if (len(time_series) - window_in - horizon + 1) <= 0:\n",
    "        raise ValueError(\"Time series too short for window size or horizon.\")\n",
    "\n",
    "    d = time_series.values\n",
    "    n_samples = len(d) - window_in - horizon + 1\n",
    "\n",
    "    X = np.empty((n_samples, window_in))\n",
    "    y = np.empty((n_samples, horizon))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        X[i] = d[i : i + window_in]\n",
    "        y[i] = d[i + window_in : i + window_in + horizon]\n",
    "\n",
    "    cols_x = [f\"x{i}\" for i in range(1, window_in + 1)]\n",
    "    cols_y = [f\"y{i}\" for i in range(1, horizon + 1)]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        np.hstack([X, y]),\n",
    "        columns=cols_x + cols_y\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "#### 4.2.2 Several window sizes comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "Now we will compare three window sizes (one day, two days and one week) to find the most efficient one. For that we will use Random Forest because it's the least sensitive to data changes and variance is reduced by averaging.\n",
    "To compare the results we will use MAE - Mean Aboslute Error - because it has low sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [24, 48, 168]   # 1 day, 2 days, 1 week\n",
    "horizons = [1, 24] # the number of time steps into the future to forecast at any point in time\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "results = []\n",
    "\n",
    "for w in window_sizes:\n",
    "\n",
    "    X, y = make_sliding_window(train_dfML[target_col].values, window_size=w, horizon=1)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1 # enable paralelism\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            \"window\": w,\n",
    "            \"fold\": fold,\n",
    "            \"MAE\": mae\n",
    "        })\n",
    "        print(f\"Window {w}, Fold {fold}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.groupby(\"window\")[\"MAE\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Mean Aboslute Error (MAE) is the lowest for 24-hour long window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_WINDOW = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_sliding_window(\n",
    "    train_dfML[target_col].values,\n",
    "    window_size=BEST_WINDOW,\n",
    "    horizon=1\n",
    ")\n",
    "\n",
    "print(\"One Day Sliding Window Created for 1 step\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw_one_day_multi_step = slideWindow2(\n",
    "        train_dfML[target_col],\n",
    "        window_in=BEST_WINDOW,\n",
    "        horizon=24\n",
    "    )\n",
    "\n",
    "print(\"One Day Sliding Window Created for 24 step\") # TO BE CREATED LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### 4.2.3 Model selection and tuning\n",
    "\n",
    "Model selection will be performed for 1-step predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_stage1 = {\n",
    "    \"LinearRegression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]),\n",
    "\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "\n",
    "    \"KNN\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", KNeighborsRegressor())\n",
    "    ]),\n",
    "\n",
    "    \"SVR\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", SVR())\n",
    "    ]),\n",
    "\n",
    "    \"Bagging\": BaggingRegressor(random_state=42),\n",
    "\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.maximum(np.abs(y_true), eps)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    \"mae\": \"neg_mean_absolute_error\",\n",
    "    \"rmse\": make_scorer(\n",
    "        lambda y, yhat: np.sqrt(mean_squared_error(y, yhat)),\n",
    "        greater_is_better=False\n",
    "    ),\n",
    "    \"mape\": make_scorer(\n",
    "        safe_mape,\n",
    "        greater_is_better=False\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "stage1_results = []\n",
    "\n",
    "for name, model in models_stage1.items():\n",
    "    fold_mae = []\n",
    "    fold_rmse = []\n",
    "    fold_mape = []\n",
    "\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "        print(f\"  Fold {fold}/5\")\n",
    "\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        fold_mae.append(mean_absolute_error(y_val, y_pred))\n",
    "        fold_rmse.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        fold_mape.append(\n",
    "            np.mean(np.abs((y_val - y_pred) / np.maximum(np.abs(y_val), 1e-8)))\n",
    "        )\n",
    "\n",
    "    stage1_results.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": np.mean(fold_mae),\n",
    "        \"RMSE\": np.mean(fold_rmse),\n",
    "        \"MAPE\": np.mean(fold_mape),\n",
    "    })\n",
    "\n",
    "    print(f\"  → MAE:  {np.mean(fold_mae):.3f}\")\n",
    "    print(f\"  → RMSE: {np.mean(fold_rmse):.3f}\")\n",
    "    print(f\"  → MAPE: {np.mean(fold_mape):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stage1 = (\n",
    "    pd.DataFrame(stage1_results)\n",
    "      .sort_values(by=[\"MAE\", \"RMSE\", \"MAPE\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "print(results_stage1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Before tuning:\n",
    "\n",
    "| Model            | MAE    | RMSE   | MAPE   |\n",
    "| ---------------- | ------ | ------ | ------ |\n",
    "| LinearRegression | 2.0272 | 3.1191 | 0.0561 |\n",
    "| RandomForest     | 2.2178 | 3.4819 | 0.0657 |\n",
    "| LightGBM         | 2.2992 | 3.5899 | 0.0693 |\n",
    "| GradientBoosting | 2.3319 | 3.6322 | 0.0688 |\n",
    "| Bagging          | 2.3822 | 3.6588 | 0.0676 |\n",
    "| XGBoost          | 2.4362 | 3.7724 | 0.0773 |\n",
    "| SVR              | 3.1489 | 5.6502 | 0.1291 |\n",
    "| DecisionTree     | 3.4551 | 5.2995 | 0.1020 |\n",
    "| KNN              | 3.9266 | 5.5235 | 0.1241 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "#### 4.2.4 Hyperparameter tuning\n",
    "\n",
    "Hyperparameter tuning will be performed for 1-step predictions.\n",
    "\n",
    "svr musi byc sekwencyjnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"DecisionTree\": {\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "        \"min_samples_leaf\": [1, 5, 10]\n",
    "    },\n",
    "\n",
    "    \"KNN\": {\n",
    "        \"model__n_neighbors\": [3, 5, 10, 20]\n",
    "    },\n",
    "\n",
    "    \"SVR\": {\n",
    "        \"model__C\": [1, 10],\n",
    "        \"model__gamma\": [\"scale\"]\n",
    "    },\n",
    "\n",
    "    \"Bagging\": {\n",
    "        \"n_estimators\": [50, 100]\n",
    "    },\n",
    "\n",
    "    \"GradientBoosting\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"max_depth\": [3, 5]\n",
    "    },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [5, 10, None]\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"max_depth\": [3, 5]\n",
    "    },\n",
    "\n",
    "    \"LightGBM\": {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"num_leaves\": [31, 63]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = []\n",
    "best_estimators = {}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for name, model in models_stage1.items():\n",
    "    print(f\"\\nTuning model: {name}\")\n",
    "\n",
    "    if name == \"LinearRegression\":\n",
    "        print(\"No tuning perfomed\")\n",
    "        continue\n",
    "\n",
    "    if name in [\"SVR\", \"LightGBM\"]:\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grids.get(name, {}),\n",
    "            cv=tscv,\n",
    "            scoring=scoring,     \n",
    "            refit=\"mae\",         \n",
    "            n_jobs=1\n",
    "        )\n",
    "    elif name in [\"RandomForest\", \"XGBoost\"]:\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grids.get(name, {}),\n",
    "            cv=tscv,\n",
    "            scoring=scoring,    \n",
    "            refit=\"mae\"\n",
    "        )\n",
    "    else:\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grids.get(name, {}),\n",
    "            cv=tscv,\n",
    "            scoring=scoring,     \n",
    "            refit=\"mae\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_idx = grid.best_index_\n",
    "\n",
    "    best_mae = -grid.cv_results_[\"mean_test_mae\"][best_idx]\n",
    "    best_rmse = -grid.cv_results_[\"mean_test_rmse\"][best_idx]\n",
    "    best_mape = -grid.cv_results_[\"mean_test_mape\"][best_idx]\n",
    "\n",
    "    tuning_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best_MAE\": best_mae,\n",
    "        \"Best_RMSE\": best_rmse,\n",
    "        \"Best_MAPE\": best_mape,\n",
    "        \"Best_Params\": grid.best_params_\n",
    "    })\n",
    "\n",
    "    best_estimators[name] = grid.best_estimator_\n",
    "\n",
    "    print(f\"  → MAE:  {best_mae:.3f}\")\n",
    "    print(f\"  → RMSE: {best_rmse:.3f}\")\n",
    "    print(f\"  → MAPE: {best_mape:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tuning = (\n",
    "    pd.DataFrame(tuning_results)\n",
    "      .sort_values(\"Best_MAE\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "print(results_tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "After tuning:\n",
    "\n",
    "| Model            | MAE       | RMSE      | MAPE       | Best Parameters                                       |\n",
    "| ---------------- | --------- | --------- | ---------- | ----------------------------------------------------- |\n",
    "| **RandomForest** | **2.211** | **3.470** | **0.0658** | `max_depth=None, n_estimators=200`                    |\n",
    "| Bagging          | 2.221     | 3.479     | 0.0654     | `n_estimators=100`                                    |\n",
    "| LightGBM         | 2.263     | 3.544     | 0.0684     | `learning_rate=0.05, n_estimators=200, num_leaves=31` |\n",
    "| GradientBoosting | 2.274     | 3.621     | 0.0703     | `learning_rate=0.05, max_depth=5, n_estimators=200`   |\n",
    "| XGBoost          | 2.279     | 3.595     | 0.0707     | `learning_rate=0.05, max_depth=5, n_estimators=200`   |\n",
    "| SVR              | 2.527     | 4.664     | 0.0832     | `C=10, gamma=scale`                                   |\n",
    "| DecisionTree     | 2.717     | 4.049     | 0.0793     | `max_depth=10, min_samples_leaf=10`                   |\n",
    "| KNN              | 3.875     | 5.434     | 0.1281     | `n_neighbors=10`                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "Among the evaluated machine-learning models, Random Forest achieved the best overall performance, obtaining the lowest MAE, RMSE, and MAPE. Ensemble methods generally outperformed single learners, while distance-based (KNN) and kernel-based (SVR) models performed worse. \n",
    "\n",
    "***Comparison with the baseline: Seasonal Naïve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_last_value_cv(X_train, y_train, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    maes, rmses, mapes = [], [], []\n",
    "\n",
    "    for _, val_idx in tscv.split(X_train):\n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "\n",
    "        # Naive forecast = last value in the window\n",
    "        y_pred = X_val[:, -1]\n",
    "\n",
    "        maes.append(mean_absolute_error(y_val, y_pred))\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        mapes.append(\n",
    "            np.mean(np.abs((y_val - y_pred) / np.maximum(np.abs(y_val), 1e-8)))\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"Model\": \"Naive (Last value)\",\n",
    "        \"MAE\": np.mean(maes),\n",
    "        \"RMSE\": np.mean(rmses),\n",
    "        \"MAPE\": np.mean(mapes),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = naive_last_value_cv(X_train, y_train)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONAL_LAG = 24   # change if your data is different\n",
    "\n",
    "def seasonal_naive_cv(X, y, seasonal_lag=24, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    maes, rmses, mapes = [], [], []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_val = X[val_idx]\n",
    "        y_val = y[val_idx]\n",
    "\n",
    "        # Seasonal naive forecast\n",
    "        # Take value from t - seasonal_lag inside the window\n",
    "        y_pred = X_val[:, -seasonal_lag]\n",
    "\n",
    "        maes.append(mean_absolute_error(y_val, y_pred))\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        mapes.append(\n",
    "            np.mean(\n",
    "                np.abs((y_val - y_pred) / np.maximum(np.abs(y_val), 1e-8))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"Model\": \"SeasonalNaive\",\n",
    "        \"MAE\": np.mean(maes),\n",
    "        \"RMSE\": np.mean(rmses),\n",
    "        \"MAPE\": np.mean(mapes),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = seasonal_naive_cv(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    seasonal_lag=24,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "baseline_df = pd.DataFrame([baseline_result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final results\n",
    "\n",
    "results_before = (\n",
    "    pd.DataFrame(stage1_results)\n",
    "    .sort_values(by=[\"MAE\", \"RMSE\", \"MAPE\"])\n",
    "    .rename(columns={\n",
    "        \"MAE\": \"MAE_before\",\n",
    "        \"RMSE\": \"RMSE_before\",\n",
    "        \"MAPE\": \"MAPE_before\"\n",
    "    })\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "results_after = (\n",
    "    pd.DataFrame(tuning_results)\n",
    "    .rename(columns={\n",
    "        \"Best_MAE\": \"MAE_after\",\n",
    "        \"Best_RMSE\": \"RMSE_after\",\n",
    "        \"Best_MAPE\": \"MAPE_after\"\n",
    "    })\n",
    "    .drop(columns=\"Best_Params\")\n",
    ")\n",
    "\n",
    "comparison_table = (\n",
    "    results_before\n",
    "    .merge(results_after, on=\"Model\", how=\"left\")\n",
    "    .sort_values(\"MAE_after\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_table = pd.concat(\n",
    "    [baseline_df, comparison_table],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "final_table = final_table.sort_values(\n",
    "    by=[\"MAE_after\", \"RMSE_after\", \"MAPE_after\"],\n",
    "    na_position=\"last\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFinal results (including baseline):\")\n",
    "print(final_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "#### 4.2.5 Perform predictions\n",
    "\n",
    "##### 4.2.5.1 Single-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "##### 4.2.5.2 Multi-step (24-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "#### 4.2.5.3 Plots comparing predictions vs. real test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "### 4.3 Deep Learning Models\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "WINDOW_SIZE = 48  # Lookback period (e.g., 48 hours)\n",
    "BATCH_SIZE = 32   # Hyperparameter to tune\n",
    "EPOCHS = 50       # Max epochs\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train_dfML)\n",
    "test_scaled = scaler.transform(test_dfML)\n",
    "\n",
    "X_train_full, y_train_full = make_sliding_window(train_scaled, WINDOW_SIZE, horizon=1)\n",
    "X_test, y_test = make_sliding_window(test_scaled, WINDOW_SIZE, horizon=1)\n",
    "\n",
    "y_train_full = y_train_full[:, 0]\n",
    "y_test = y_test[:, 0]\n",
    "\n",
    "val_split = int(len(X_train_full) * 0.8)\n",
    "X_train, y_train = X_train_full[:val_split], y_train_full[:val_split]\n",
    "X_val, y_val = X_train_full[val_split:], y_train_full[val_split:]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. LSTM Model Definition\n",
    "# ==========================================\n",
    "def build_lstm_model(input_shape):\n",
    "    lstm_model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    lstm_model.add(Input(shape=input_shape))\n",
    "\n",
    "    # LSTM Layer 1\n",
    "    lstm_model.add(LSTM(units=50, return_sequences=True))\n",
    "    lstm_model.add(Dropout(0.2))\n",
    "\n",
    "    # LSTM Layer 2\n",
    "    lstm_model.add(LSTM(units=25, return_sequences=False))\n",
    "    lstm_model.add(Dropout(0.2))\n",
    "\n",
    "    # Output Layer\n",
    "    lstm_model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    lstm_model.compile(optimizer=optimizer, loss='mse')\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "lstm_model = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Training\n",
    "# ==========================================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Evaluation & Metrics\n",
    "# ==========================================\n",
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "\n",
    "def inverse_transform_target(pred_array, scaler, n_features):\n",
    "    dummy = np.zeros((len(pred_array), n_features))\n",
    "    dummy[:, 0] = pred_array.flatten()\n",
    "    return scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "y_pred_real = inverse_transform_target(y_pred_scaled, scaler, train_dfML.shape[1])\n",
    "y_test_real = inverse_transform_target(y_test.reshape(-1, 1), scaler, train_dfML.shape[1])\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "non_zero_mask = y_test_real != 0\n",
    "mape = np.mean(np.abs((y_test_real[non_zero_mask] - y_pred_real[non_zero_mask]) / y_test_real[non_zero_mask])) * 100\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_real[:200], label='Actual Price', color='blue')\n",
    "plt.plot(y_pred_real[:200], label='Predicted Price', color='red', linestyle='--')\n",
    "plt.title('LSTM Forecast: Actual vs Predicted (First 200 Hours)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "RESULTS ANALYSIS\n",
    "\n",
    "The Long Short-Term Memory (LSTM) network demonstrated strong performance in capturing the temporal dependencies of electricity prices, achieving an MAE of 0.0959 and an RMSE of 0.1532. The high MAPE (82.15%) is attributed to the presence of near-zero prices in the test set rather than poor predictive quality.\n",
    "\n",
    "Visually, the LSTM predictions track the actual test data very closely, successfully replicating the daily seasonality and general trends of the market. A slight \"reaction delay\" was observed, which is expected in autoregressive models that rely on immediate past windows. While the model handles standard volatility well, it occasionally struggles with extreme, rapid fluctuations (drastic down-and-up movements), where it tends to smooth the curve rather than capturing the full amplitude of the sharpest spikes.\n",
    "\n",
    "The training process was stable, with training and validation loss curves converging naturally. The EarlyStopping mechanism was not triggered, and the optimal model weights were recorded at epoch 49 (out of 50). This continuous improvement throughout the training cycle indicates that the model had appropriate capacity and learning rate settings for this dataset, avoiding both premature convergence and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. GRU Model Definition\n",
    "# ==========================================\n",
    "def build_gru_model(input_shape):\n",
    "    gru_model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    gru_model.add(Input(shape=input_shape))\n",
    "\n",
    "    # GRU Layer 1\n",
    "    gru_model.add(GRU(units=50, return_sequences=True))\n",
    "    gru_model.add(Dropout(0.2))\n",
    "\n",
    "    # GRU Layer 2\n",
    "    gru_model.add(GRU(units=25, return_sequences=False))\n",
    "    gru_model.add(Dropout(0.2))\n",
    "\n",
    "    # Output Layer\n",
    "    gru_model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    gru_model.compile(optimizer=optimizer, loss='mse')\n",
    "    return gru_model\n",
    "\n",
    "\n",
    "gru_model = build_gru_model(input_shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Training\n",
    "# ==========================================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_gru.history['loss'], label='Training Loss')\n",
    "plt.plot(history_gru.history['val_loss'], label='Validation Loss')\n",
    "plt.title('GRU Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Evaluation\n",
    "# ==========================================\n",
    "y_pred_scaled_gru = gru_model.predict(X_test)\n",
    "\n",
    "y_pred_real_gru = inverse_transform_target(y_pred_scaled_gru, scaler, train_dfML.shape[1])\n",
    "\n",
    "mae_gru = mean_absolute_error(y_test_real, y_pred_real_gru)\n",
    "rmse_gru = np.sqrt(mean_squared_error(y_test_real, y_pred_real_gru))\n",
    "non_zero_mask = y_test_real != 0\n",
    "mape_gru = np.mean(np.abs((y_test_real[non_zero_mask] - y_pred_real_gru[non_zero_mask]) / y_test_real[non_zero_mask])) * 100\n",
    "\n",
    "print(f\"\\nGRU Model Performance:\")\n",
    "print(f\"MAE:  {mae_gru:.4f}\")\n",
    "print(f\"RMSE: {rmse_gru:.4f}\")\n",
    "print(f\"MAPE: {mape_gru:.4f}%\")\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_real[:200], label='Actual Price', color='black', alpha=0.7)\n",
    "plt.plot(y_pred_real[:200], label='LSTM Prediction', color='red', linestyle='--', alpha=0.7)\n",
    "plt.plot(y_pred_real_gru[:200], label='GRU Prediction', color='green', linestyle='-.', alpha=0.7)\n",
    "plt.title('Comparison: LSTM vs GRU vs Actual (First 200 Hours)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "GRU Analysis\n",
    "\n",
    "The Gated Recurrent Unit (GRU) model outperformed the LSTM on this dataset, achieving a lower MAE of 0.0822 (vs. 0.0959 for LSTM) and RMSE of 0.1381 (vs. 0.1532 for LSTM). This indicates that the GRU provided a slightly more accurate fit to the test data overall. As expected, the training time was shorter, 14 minutes insted of 18 for LSTM.\n",
    "\n",
    "Visually, the GRU predictions followed a very similar trajectory to the LSTM, closely tracking the actual price curve. This suggests that both recurrent architectures successfully captured the underlying temporal patterns of the electricity market. The fact that the simpler GRU architecture yielded better metrics suggests that the additional complexity of the LSTM (extra gates and separate cell state) was not necessary for this specific 1-step forecast task and may have introduced slight inefficiencies or noise.\n",
    "\n",
    "Similar to the LSTM, the GRU showed stable learning and continued to improve throughout the training process, with the best model weights saved at the final epoch 50. This confirms that the model did not overfit and could potentially benefit from an even longer training duration, though the current performance is already superior to the LSTM baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "24-Step Ahead Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Data Preparation for 24-Step Ahead\n",
    "# ==========================================\n",
    "def create_24h_window(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - 24 + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size: i + window_size + 24, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "WINDOW_SIZE = 48\n",
    "\n",
    "X_train_24, y_train_24 = create_24h_window(train_scaled, WINDOW_SIZE)\n",
    "X_test_24, y_test_24 = create_24h_window(test_scaled, WINDOW_SIZE)\n",
    "\n",
    "val_split_24 = int(len(X_train_24) * 0.8)\n",
    "X_train_sub_24, y_train_sub_24 = X_train_24[:val_split_24], y_train_24[:val_split_24]\n",
    "X_val_24, y_val_24 = X_train_24[val_split_24:], y_train_24[val_split_24:]\n",
    "\n",
    "print(f\"Input Shape: {X_train_sub_24.shape}\")\n",
    "print(f\"Target Shape: {y_train_sub_24.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. LSTM Model for 24-Step Output\n",
    "# ==========================================\n",
    "model_24 = Sequential()\n",
    "# Input Layer\n",
    "model_24.add(Input(shape=(X_train_sub_24.shape[1], X_train_sub_24.shape[2])))\n",
    "\n",
    "# LSTM24 Layer 1\n",
    "model_24.add(LSTM(128, return_sequences=True))\n",
    "model_24.add(Dropout(0.2))\n",
    "\n",
    "# LSTM24 Layer 2\n",
    "model_24.add(LSTM(68, return_sequences=False))\n",
    "model_24.add(Dropout(0.2))\n",
    "\n",
    "# OUTPUT LAYER\n",
    "model_24.add(Dense(24))\n",
    "\n",
    "model_24.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Training\n",
    "# ==========================================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_24 = model_24.fit(\n",
    "    X_train_sub_24, y_train_sub_24,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val_24, y_val_24),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Evaluation\n",
    "# ==========================================\n",
    "y_pred_scaled_24 = model_24.predict(X_test_24)\n",
    "\n",
    "n_samples = y_pred_scaled_24.shape[0]\n",
    "n_steps = 24\n",
    "n_features = train_dfML.shape[1]\n",
    "\n",
    "flat_preds = y_pred_scaled_24.reshape(-1, 1)\n",
    "dummy_preds = np.zeros((len(flat_preds), n_features))\n",
    "dummy_preds[:, 0] = flat_preds.flatten()\n",
    "flat_real_preds = scaler.inverse_transform(dummy_preds)[:, 0]\n",
    "\n",
    "y_pred_real_24 = flat_real_preds.reshape(n_samples, n_steps)\n",
    "\n",
    "flat_actuals = y_test_24.reshape(-1, 1)\n",
    "dummy_actuals = np.zeros((len(flat_actuals), n_features))\n",
    "dummy_actuals[:, 0] = flat_actuals.flatten()\n",
    "flat_real_actuals = scaler.inverse_transform(dummy_actuals)[:, 0]\n",
    "y_test_real_24 = flat_real_actuals.reshape(n_samples, n_steps)\n",
    "\n",
    "mae_24 = mean_absolute_error(y_test_real_24, y_pred_real_24)\n",
    "rmse_24 = np.sqrt(mean_squared_error(y_test_real_24, y_pred_real_24))\n",
    "mask_24 = y_test_real_24 != 0\n",
    "mape_24 = np.mean(np.abs((y_test_real_24[mask_24] - y_pred_real_24[mask_24]) / y_test_real_24[mask_24])) * 100\n",
    "\n",
    "print(f\"\\n24-Step Ahead Model Performance:\")\n",
    "print(f\"MAE:  {mae_24:.4f}\")\n",
    "print(f\"RMSE: {rmse_24:.4f}\")\n",
    "print(f\"MAPE: {mape_24:.4f}%\")\n",
    "\n",
    "\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(24), y_test_real_24[sample_idx], marker='o', label='Actual Day')\n",
    "plt.plot(range(24), y_pred_real_24[sample_idx], marker='x', linestyle='--', label='Predicted Day')\n",
    "plt.title(f'24-Hour Forecast for Sample #{sample_idx}')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Continuous Plot (Day-Ahead Simulation)\n",
    "# ==========================================\n",
    "\n",
    "stitched_pred = y_pred_real_24[::24].flatten()\n",
    "stitched_actual = y_test_real_24[::24].flatten()\n",
    "\n",
    "plot_hours = 240 # 10 days\n",
    "\n",
    "plot_hours = min(plot_hours, len(stitched_pred))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(stitched_actual[:plot_hours], label='Actual Price', color='blue', alpha=0.6)\n",
    "\n",
    "plt.plot(stitched_pred[:plot_hours], label='Day-Ahead Prediction (Stitched)',\n",
    "         color='orange', linestyle='--', marker='.', markersize=5)\n",
    "\n",
    "plt.title(f'Day-Ahead Forecast: Continuous View (First {plot_hours} Hours)')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Electricity Price')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "mae_stitched = mean_absolute_error(stitched_actual, stitched_pred)\n",
    "print(f\"Realistic Day-Ahead Operation MAE: {mae_stitched:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "24-Step LSTM Analysis\n",
    "\n",
    "Predicting a full 24-hour horizon proved to be significantly more challenging than the 1-step task. The model achieved an MAE of 0.2650 and RMSE of 0.3563. These errors are roughly 3x higher than the 1-step GRU/LSTM models (MAE ~0.08), reflecting the inherent difficulty of forecasting an entire day without the ability to correct course using intermediate real values.\n",
    "\n",
    "On a larger scale, the model demonstrates a basic ability to follow the general market trend, confirming it learned the overall weekly and daily seasonality. A major limitation observed is a substantial \"phase shift\" or lag. When a rapid price drop occurs, the model often predicts this drop only after the event has passed (likely with a 24-hour delay). This suggests the model is relying heavily on persistence (repeating yesterday's pattern) rather than anticipating changes based on exogenous features like weather. Visually, the predictions are frequently \"off\" from the actual values, lacking the precision seen in the 1-step models. This is consistent with the EarlyStopping occurring at epoch 15 (best weights at epoch 5), indicating the model struggled to find a complex pattern and converged quickly to a simpler, safer suboptimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64]\n",
    "results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Testing Batch Size: {bs}\")\n",
    "    model = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=bs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    results[bs] = best_val_loss\n",
    "    print(f\"Batch Size {bs} - Best Val Loss: {best_val_loss:.5f}\")\n",
    "\n",
    "print(\"Best Configuration:\", min(results, key=results.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tunable_lstm(input_shape, units, num_layers, dropout_rate=0.2):\n",
    "    tunable_lstm_model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    tunable_lstm_model.add(Input(shape=input_shape))\n",
    "\n",
    "    # First Layer (Always present)\n",
    "    return_seq = (num_layers > 1)\n",
    "    tunable_lstm_model.add(LSTM(units=units, return_sequences=return_seq))\n",
    "    tunable_lstm_model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Second Layer (Conditional)\n",
    "    if num_layers == 2:\n",
    "        tunable_lstm_model.add(LSTM(units=units // 2, return_sequences=False))\n",
    "        tunable_lstm_model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output Layer\n",
    "    tunable_lstm_model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    tunable_lstm_model.compile(optimizer=optimizer, loss='mse')\n",
    "    return tunable_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Hyperparameter Tuning: Units & Layers\n",
    "# ==========================================\n",
    "\n",
    "units_options = [32, 64, 128]\n",
    "layers_options = [1, 2]\n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for units in units_options:\n",
    "    for layers in layers_options:\n",
    "        print(f\"Testing: {units} Units | {layers} Layers...\")\n",
    "\n",
    "        model = build_tunable_lstm(\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            units=units,\n",
    "            num_layers=layers\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=30,\n",
    "            batch_size=32, # Using your fixed best batch size\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "        tuning_results.append({\n",
    "            'units': units,\n",
    "            'layers': layers,\n",
    "            'val_loss': best_val_loss\n",
    "        })\n",
    "\n",
    "        print(f\"   -> Result: Best Val Loss = {best_val_loss:.5f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Tuning Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(tuning_results)\n",
    "\n",
    "results_df = results_df.sort_values(by='val_loss', ascending=True)\n",
    "\n",
    "print(\"\\nTop 3 Configurations:\")\n",
    "print(results_df.head(3))\n",
    "\n",
    "best_config = results_df.iloc[0]\n",
    "print(f\"\\nWINNER: LSTM with {int(best_config['units'])} Units and {int(best_config['layers'])} Layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "Hyperparameter Analysis\n",
    "\n",
    "The tuning process revealed a clear preference for a simpler architecture. The configuration with 32 units achieved the lowest validation loss (0.000856), outperforming larger configurations like 128 units (0.000904) or 64 units (0.000981). The differences are extremly slow and multiple test showed that the results may vary depending on test.\n",
    "\n",
    "The fact that 1-layer models consistently outperformed 2-layer models (which did not appear in the top 3) suggests that the features in this dataset (price, weather, generation) are strong enough that a deeper, more complex network is not required. Adding a second layer likely introduced unnecessary noise or optimization difficulties rather than learning new abstractions.\n",
    "\n",
    "Selecting the 32-unit model not only improves accuracy but also significantly reduces the computational cost, making the model faster to train and lighter to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Seasonal Naive\n",
    "# ==========================================\n",
    "seasonality = 168\n",
    "y_naive_pred = []\n",
    "y_naive_real = []\n",
    "\n",
    "test_values = test_dfML.iloc[:, 0].values # Assuming price is col 0\n",
    "\n",
    "for i in range(seasonality, len(test_values)):\n",
    "    y_naive_pred.append(test_values[i - seasonality])\n",
    "    y_naive_real.append(test_values[i])\n",
    "\n",
    "mae_naive = mean_absolute_error(y_naive_real, y_naive_pred)\n",
    "rmse_naive = np.sqrt(mean_squared_error(y_naive_real, y_naive_pred))\n",
    "print(f\"Baseline (Seasonal Naive) - MAE: {mae_naive:.4f}, RMSE: {rmse_naive:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_naive_real[:240], label='Actual Price', color='tab:blue', alpha=0.7)\n",
    "plt.plot(y_naive_pred[:240], label='10-Day Recursive Forecast', color='tab:orange', linestyle='--')\n",
    "\n",
    "plt.title(f'10-Day Recursive Forecast (LSTM)')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.xlabel('Hours (0 to 240)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "Naive Analysis\n",
    "\n",
    "The Seasonal Naive baseline performed significantly worse than the Deep Learning models, yielding an MAE of 0.4179 and RMSE of 0.5468. In comparison, the best-performing model (GRU) achieved an MAE of roughly 0.08, representing an improvement of approximately 80% over this baseline. This huge gap confirms that the project's complexity was justified.\n",
    "\n",
    "The graph appears to \"predict something\" in certain areas because electricity demand does have a strong weekly structure. The areas where the prediction looks \"random\" or fails completely represent times when exogenous factors (weather changes, wind generation, holidays) shifted the price away from the historical average. Because the Naïve model blindly copies the previous week's value , it cannot account for these dynamic changes, leading to large errors whenever the weather pattern changes from one week to the next.\n",
    "\n",
    "The poor performance of the Seasonal Naïve model demonstrates that electricity prices are not merely repetitive cycles. They are driven by complex, non-linear dependencies on weather and generation data, which only the Deep Learning models (LSTM/GRU) were able to successfully learn and predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Conclusions and Recommendations\n",
    "\n",
    "### 6.1 Summary of Findings\n",
    "\n",
    "\n",
    "### 6.2 Best Model Selection\n",
    "\n",
    "### 6.3 Future Work\n",
    "\n",
    "\n",
    "### 6.4 Lessons Learned\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "## Project Notes and Team Collaboration\n",
    "\n",
    "### Team Members\n",
    "\n",
    "- Julia Kardasz 1250264\n",
    "- Mateusz Nowak 1250296\n",
    "- Emilia Pawlowska 1250230"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
